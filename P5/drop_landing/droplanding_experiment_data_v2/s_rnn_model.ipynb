{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX4Kg8DUTKWO"
   },
   "source": [
    "# <centr>Description of this project </center>\n",
    "## There are 14 subjects attending the experiments, they are indexed by following keys:\n",
    "#['sub_0','sub_1', 'sub_2', 'sub_3', 'sub_4', 'sub_5', 'sub_6', 'sub_7', 'sub_8', 'sub_9'，'sub_10', 'sub_11', 'sub_12', 'sub_13',sub_13]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "all_datasets_len={'sub_0':6951, 'sub_1':7439, 'sub_2': 7686, 'sub_3': 8678, 'sub_4':6180, 'sub_5': 6671, \n",
    "                   'sub_6': 7600, 'sub_7': 5583, 'sub_8': 6032, 'sub_9': 6508, 'sub_10': 6348, 'sub_11': 7010, 'sub_12': 8049, 'sub_13': 6248}\n",
    "all_datasets_ranges={'sub_-1':0, sub_0': 6951, 'sub_1': 14390, 'sub_2': 22076, 'sub_3': 30754, 'sub_4': 36934, 'sub_5': 43605,\n",
    "                      'sub_6': 51205, 'sub_7': 56788, 'sub_8': 62820, 'sub_9': 69328, 'sub_10': 75676, 'sub_11':82686, 'sub_12': 90735, 'sub_13': 96983}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyper parametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.4.0\n"
     ]
    }
   ],
   "source": [
    "## import necessary packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import os\n",
    "import yaml\n",
    "import h5py\n",
    "print(\"tensorflow version:\",tf.__version__)\n",
    "# load datasets in a numpy \n",
    "import fnn_model_v3 as fnn_model_v3\n",
    "\n",
    "def initParameters():\n",
    "    # hyper parameters\n",
    "    hyperparams=fnn_model_v3.hyperparams\n",
    "    \n",
    "    labels_names=[ 'L_IE', 'L_AA', 'L_FE','R_IE', 'R_AA', 'R_FE' ]\n",
    "    \n",
    "    '''\n",
    "    features_names=[\n",
    "        'L_Up_Acc_X', 'L_Up_Acc_Y', 'L_Up_Acc_Z', 'L_Up_Gyr_X', 'L_Up_Gyr_Y','L_Up_Gyr_Z',\n",
    "        'L_Up_Mag_X','L_Up_Mag_Y','L_Up_Mag_Z',\n",
    "        \n",
    "        'L_Lower_Acc_X', 'L_Lower_Acc_Y', 'L_Lower_Acc_Z', 'L_Lower_Gyr_X', 'L_Lower_Gyr_Y','L_Lower_Gyr_Z',\n",
    "        'L_Lower_Mag_X',   'L_Lower_Mag_Y','L_Lower_Mag_Z',\n",
    "        \n",
    "        'R_Up_Acc_X', 'R_Up_Acc_Y', 'R_Up_Acc_Z', 'R_Up_Gyr_X', 'R_Up_Gyr_Y','R_Up_Gyr_Z', \n",
    "        'R_Up_Mag_X', 'R_Up_Mag_Y', 'R_Up_Mag_Z',\n",
    "        \n",
    "        'R_Lower_Acc_X', 'R_Lower_Acc_Y', 'R_Lower_Acc_Z', 'R_Lower_Gyr_X', 'R_Lower_Gyr_Y','R_Lower_Gyr_Z', \n",
    "        'R_Lower_Mag_X',   'R_Lower_Mag_Y','R_Lower_Mag_Z'\n",
    "        ]    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # without magnetometer data\n",
    "    features_names=[\n",
    "        'L_Up_Acc_X', 'L_Up_Acc_Y', 'L_Up_Acc_Z', 'L_Up_Gyr_X', 'L_Up_Gyr_Y','L_Up_Gyr_Z', \n",
    "        \n",
    "        'L_Lower_Acc_X', 'L_Lower_Acc_Y', 'L_Lower_Acc_Z', 'L_Lower_Gyr_X', 'L_Lower_Gyr_Y','L_Lower_Gyr_Z', \n",
    "        \n",
    "        'R_Up_Acc_X', 'R_Up_Acc_Y', 'R_Up_Acc_Z', 'R_Up_Gyr_X', 'R_Up_Gyr_Y','R_Up_Gyr_Z', \n",
    "        \n",
    "        'R_Lower_Acc_X', 'R_Lower_Acc_Y', 'R_Lower_Acc_Z', 'R_Lower_Gyr_X', 'R_Lower_Gyr_Y','R_Lower_Gyr_Z'\n",
    "        ]    \n",
    "    \n",
    "    sub_idx=[0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "    #sub_idx=[0,1]\n",
    "    \n",
    "    columns_names=features_names+labels_names\n",
    "    hyperparams['features_num']=len(features_names)\n",
    "    hyperparams['labels_num']=len(labels_names)\n",
    "    hyperparams['features_names']=features_names;\n",
    "    hyperparams['labels_names']=labels_names\n",
    "    hyperparams['learning_rate']=10e-2\n",
    "    hyperparams['batch_size']=32\n",
    "    hyperparams['window_size']=10\n",
    "    hyperparams['epochs']=100\n",
    "    hyperparams['columns_names']=columns_names\n",
    "    hyperparams['raw_dataset_path']= './datasets_files/raw_datasets.hdf5'\n",
    "    hyperparams['sub_idx']=sub_idx\n",
    "    \n",
    "    print('sub_idxs:',sub_idx)\n",
    "    return hyperparams\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #hyperparams=initParameters()\n",
    "    #print(\"window size:\", hyperparams['window_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare dataset for training and evaluation\n",
    "## 2.1 Read raw subject data and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_normalize_data(sub_idx,scaler=None):\n",
    "    \n",
    "    #**** Single subject test\n",
    "    if(isinstance(sub_idx,int)):\n",
    "        start=fnn_model_v3.all_datasets_ranges['sub_'+str(sub_idx-1)]\n",
    "        end=fnn_model_v3.all_datasets_ranges['sub_'+str(sub_idx)]\n",
    "        series=fnn_model_v3.read_rawdata(range(start,end),hyperparams['columns_names'],hyperparams['raw_dataset_path'])\n",
    "        print(\"Raw data of subject {:}, rows from {:} to {:}\".format(sub_idx,start,end))\n",
    "    \n",
    "    #**** Multiple subject data indexed by numbers\n",
    "    if(isinstance(sub_idx,list) and isinstance(sub_idx[0],int)):\n",
    "        start_sub_num=int(sub_idx[0])\n",
    "        end_sub_num=int(sub_idx[-1])\n",
    "        start=fnn_model_v3.all_datasets_ranges['sub_'+str(start_sub_num-1)]\n",
    "        end=fnn_model_v3.all_datasets_ranges['sub_'+str(end_sub_num)]\n",
    "        series=fnn_model_v3.read_rawdata(range(start,end),hyperparams['columns_names'],hyperparams['raw_dataset_path'])\n",
    "        print(\"Raw data of subject {:}, rows from {:} to {:}\".format(sub_idx,start,end))\n",
    "    \n",
    "    #**** Multiple subject data indexed by \"sub_num\"\n",
    "    if(isinstance(sub_idx,list) and isinstance(sub_idx[0],str)):\n",
    "        series_temp=[]\n",
    "        for idx in sub_idx:\n",
    "            assert(isinstance(idx,str))\n",
    "            series_temp.append(fnn_model_v3.read_rawdata(idx,hyperparams['columns_names'],hyperparams['raw_dataset_path']))\n",
    "        series=np.concatenate(series_temp,axis=0)\n",
    "        print(\"Raw data of subject {:}\".format(sub_idx))\n",
    "    # load dataset\n",
    "    print('Loaded dataset shape:',series.shape)\n",
    "    \n",
    "    #Normalization data\n",
    "    if scaler==None:\n",
    "        scaler=StandardScaler()\n",
    "    scaler.fit(series)\n",
    "    scaled_series=scaler.transform(series.astype(np.float32),copy=True)\n",
    "    \n",
    "    # Get the initial stage data of every subjects\n",
    "    init_stage_sub_ranges={keys:range(fnn_model_v3.all_datasets_ranges['sub_'+str(int(keys[4:])-1)],fnn_model_v3.all_datasets_ranges['sub_'+str(int(keys[4:])-1)]+300)\n",
    "                  for keys in ['sub_'+str(idx) for idx in range(14)]}\n",
    "    init_stage_sub_data={keys:fnn_model_v3.read_rawdata(values,hyperparams['columns_names'],hyperparams['raw_dataset_path']) for keys, values in init_stage_sub_ranges.items()}\n",
    "    # Normalization of init-stage data\n",
    "    scaled_init_stage_sub_data={keys:scaler.transform(values).mean(axis=0,keepdims=True) \n",
    "                                for keys, values in init_stage_sub_data.items()}\n",
    "    #empirically_orientation=[-2.32, 2.77,-3.5,  -8.65, 4.0, 3.01, -0.22, -2.11, 5.29, -9.47, -2.85, -1.33]\n",
    "    return scaled_series, scaler\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #scaled_series,scaler=load_normalize_data(sub_idx=['sub_1','sub_2','sub_4'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Split datasets into train, valid and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(scaled_series,sub_idx):\n",
    "    #1) Split raw dataset into train, valid and test dataset\n",
    "    if(isinstance(sub_idx,int)):# split single subject data \n",
    "        all_train_split={'sub_0':5600,'sub_1':6000,'sub_2':6400,'sub_3':7500,\n",
    "                         'sub_4':4900,'sub_5':6000,'sub_6':4500,'sub_7':4500,\n",
    "                         'sub_8':4900,'sub_9':5100,'sub_10':5000,'sub_11':5700,'sub_12':6400,'sub_13':4000}\n",
    "        all_valid_split={'sub_0':6500,'sub_1':6800,'sub_2':7000,'sub_3':8100,\n",
    "                         'sub_4':5600,'sub_5':6900,'sub_6':5100,'sub_7':5000,\n",
    "                         'sub_8':5600,'sub_9':5900,'sub_10':5800,'sub_11':6200,'sub_12':7200, 'sub_13':4900}\n",
    "        train_split = all_train_split['sub_'+str(sub_idx)]\n",
    "        valid_split= all_valid_split['sub_'+str(sub_idx)]\n",
    "    if(isinstance(sub_idx,list)):# split multiple subject data, using leave-one-out cross-validtion\n",
    "        train_split=scaled_series.shape[0]-3000\n",
    "        valid_split=scaled_series.shape[0]-2000\n",
    "    \n",
    "    xy_train = scaled_series[:train_split,:]\n",
    "    xy_valid = scaled_series[train_split:valid_split,:]\n",
    "    xy_test = scaled_series[valid_split:,:]\n",
    "    shuffle_buffer_size = 6000\n",
    "    \n",
    "    \n",
    "    # Sensor segment calibration transfer process\n",
    "    Init_stage_calibration=False\n",
    "    if(Init_stage_calibration):\n",
    "        np.random.seed(101)\n",
    "        transfer_weight=np.random.randn(features_num,features_num)\n",
    "        transfer_temp=np.dot(scaled_init_stage_sub_data['sub_'+str(sub_idx)][:,:features_num],transfer_weight)\n",
    "        xy_train[:,:features_num]=xy_train[:,:features_num]+np.tanh(transfer_temp)\n",
    "        xy_valid[:,:features_num]=xy_valid[:,:features_num]+np.tanh(transfer_temp)\n",
    "        xy_test[:,:features_num]=xy_test[:,:features_num]+np.tanh(transfer_temp)\n",
    "        # init info fom calibration stage \n",
    "        xy_train_init=scaled_init_stage_sub_data['sub_'+str(sub_idx)]*np.ones(xy_train.shape)\n",
    "        xy_valid_init=scaled_init_stage_sub_data['sub_'+str(sub_idx)]*np.ones(xy_valid.shape)\n",
    "        xy_test_init=scaled_init_stage_sub_data['sub_'+str(sub_idx)]*np.ones(xy_test.shape)\n",
    "    \n",
    "    print(\"Subject {:} dataset\".format(sub_idx))\n",
    "    print(\"xy_train shape:\",xy_train.shape)\n",
    "    print(\"xy valid shape:\",xy_valid.shape)\n",
    "    print(\"xy_test shape:\",xy_test.shape)\n",
    "    \n",
    "    return xy_train, xy_valid,xy_test\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #xy_train, xy_valid, xy_test = split_dataset(scaled_series,sub_idx=[1,2])\n",
    "    #hyperparams['train_sub_idx']=[1,2,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Transfer data into dataset for feeding to neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "id": "lJwUUZscnG38"
   },
   "outputs": [],
   "source": [
    "\n",
    "# packing data into windows \n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    #series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:,:-6], w[:,-6:]))\n",
    "    ds=ds.batch(batch_size).prefetch(1)\n",
    "    return ds\n",
    "\n",
    "# model prediction\n",
    "def model_forecast(model, series, window_size, batch_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series[:,:-6])\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    #print(list(ds.as_numpy_iterator())[0])\n",
    "    # model_prediction\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # transfer the data into dataset for feeding to neural network\n",
    "    pass\n",
    "    #train_set = windowed_dataset(xy_train, hyperparams['window_size'], hyperparams['batch_size'], shuffle_buffer=1000)\n",
    "    #valid_set = windowed_dataset(xy_valid, hyperparams['window_size'], hyperparams['batch_size'], shuffle_buffer=1000)\n",
    "    #train_set_init = tf.data.Dataset.from_tensor_slices(xy_train_init).batch(1).map(lambda x: (x[:,:-6],x[:,-6:]))\n",
    "    #valid_set_init = tf.data.Dataset.from_tensor_slices(xy_valid_init).batch(1).map(lambda x: (x[:,:-6],x[:,-6:]))\n",
    "    #test_set_init = windowed_dataset(xy_test_init,window_size,batch_size,shuffle_buffer_size)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display datasets\n",
    "\n",
    "#datasets_ranges=(fnn_model_v3.all_datasets_ranges['sub_'+str(sub_idx-1)],fnn_model_v3.all_datasets_ranges['sub_'+str(sub_idx)])\n",
    "#fnn_model_v3.display_rawdatase(series[31000:34000,:], columns_names, norm_type=None, raw_datasets_path=raw_dataset_path,plot_title='sub_'+str(sub_idx))\n",
    "#fnn_model_v3.display_rawdatase(scaled_series[6000:6250,:], columns_names, norm_type=None, raw_datasets_path=raw_dataset_path,plot_title='sub_'+str(sub_idx))\n",
    "#fnn_model_v3.display_rawdatase(scaler.inverse_transform(scaled_series[6000:6250,:]), columns_names, norm_type=None, raw_datasets_path=raw_dataset_path)\n",
    "#fnn_model_v3.display_rawdatase(datasets_ranges, columns_names, norm_type='mean_std', raw_datasets_path=raw_dataset_path,plot_title='raw data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model definition\n",
    "## 3.1 Model v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definne\n",
    "def model_v1(hyperparams):\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
    "                          strides=1, padding=\"causal\",\n",
    "                          activation=\"relu\",\n",
    "                          input_shape=[None, hyperparams['features_num']]),\n",
    "      tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "      tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "      #tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(60, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(30),\n",
    "      tf.keras.layers.Dense(6)\n",
    "      #tf.keras.layers.Lambda(lambda x: x *180)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Callback class\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epcoh, logs={}):\n",
    "        if(logs.get('loss')<0.003):\n",
    "            print('\\nLoss is low so cancelling training!')\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #model=model_v1(hyperparams)\n",
    "    #print(model.summary())\n",
    "#training_folder=fnn_model_v3.create_training_files(hyperparams=fnn_model_v3.hyperparams)\n",
    "#model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义组合神经网络，多个子网络\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def model_v2():\n",
    "    print(tf.__version__)\n",
    "    ####定义一个方便构造常规 Sequential() 网络的函数\n",
    "    def DNN_A_Graph(x,n_input=36,n_output=6,name='Transfer_graph'):\n",
    "        tf.random.set_seed(50)\n",
    "        np.random.seed(50)\n",
    "        he = tf.initializers.he_normal()\n",
    "        elu = tf.nn.elu\n",
    "        x=Dense(n_input, kernel_initializer=he, activation=elu,name=name+'_1')(x)\n",
    "        x=Dense(n_output,kernel_initializer=he, activation=elu,name=name+'_2')(x)\n",
    "        return(x)\n",
    "    \n",
    "    def DNN_B_Graph(x,n_input=36,n_output=6,name='Main_graph'):\n",
    "        ##\n",
    "        x=tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
    "                          strides=1, padding=\"causal\",\n",
    "                          activation=\"relu\",\n",
    "                          input_shape=[window_size, features_num],name='Conv1D')(x)\n",
    "        x=tf.keras.layers.LSTM(60, return_sequences=True,name='lstm_1')(x)\n",
    "        x=tf.keras.layers.LSTM(60, return_sequences=True,name='lstm_2')(x)\n",
    "        x=tf.keras.layers.Flatten()(x)\n",
    "        x=tf.keras.layers.Dense(60, activation=\"relu\")(x)\n",
    "        x=tf.keras.layers.Dense(30)(x)\n",
    "        x=tf.keras.layers.Dense(6)(x)\n",
    "        return(x)\n",
    "        \n",
    "        \n",
    "    #### 构造并联网络图\n",
    "    ##需要并联的两个网络的输入\n",
    "    input_a=tf.keras.layers.Input(shape=[features_num],name='Input_A')\n",
    "    input_b=tf.keras.layers.Input(shape=[hyperparams['window_size'],features_num],name='Input_B')\n",
    "    window_size=hyperparams['window_size']\n",
    "    ##构造两个需要并联的子网络结构\n",
    "    dnn_a=DNN_A_Graph(input_a,n_input=features_num,n_output=labels_num,name=\"DNN_A\")\n",
    "    dnn_b=DNN_B_Graph(input_b,n_input=features_num,n_output=labels_num,name=\"DNN_B\")\n",
    "    ##concat操作\n",
    "    concat=tf.keras.layers.concatenate([dnn_a,dnn_b],axis=-1,name=\"Concat_Layer\")\n",
    "    ##在concat基础上继续添加一些层\n",
    "    output=Dense(labels_num,name=\"Output_Layer\")(concat)\n",
    "    ##这一步很关键：这一步相当于把输入和输出对应起来，形成系统认识的一个完整的图。\n",
    "    model_v2=Model(inputs=[input_a,input_b],outputs=[output])\n",
    "    model_v2.get_layer('DNN_A_1').trainable=False\n",
    "    model_v2.get_layer('DNN_A_2').trainable=False\n",
    "    \n",
    "    ##网络的其他组件\n",
    "    optimizer=tf.keras.optimizers.Adam()\n",
    "    loss_fn=tf.keras.losses.mean_squared_error\n",
    "    model_v2.compile(loss=loss_fn,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'],\n",
    "                  )\n",
    "    \n",
    "    model_v2.summary()\n",
    "    #### 训练和测试：这里的x1,x2,对应前述的input_a和input_b\n",
    "    #model.fit(x=[x1,x2],y=y,epochs=10,batch_size=500,verbose=2)\n",
    "    \n",
    "    #model.evaluate(x=[x1_test,x2_test],y=y_test,verbose=2)\n",
    "    return model_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AclfYY3Mn6Ph",
    "outputId": "dd1fef93-d819-4d56-df20-330169907e16",
    "tags": []
   },
   "source": [
    "# find optimal lr\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "history = model.fit(train_set, epochs=150, callbacks=[lr_schedule])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "vVcKmg7Q_7rD",
    "outputId": "5e9b8029-e996-4a2b-e016-666c69865b11"
   },
   "source": [
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1, 0, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Train model\n",
    "## 4.1 Train model V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QsksvkcXAAgq",
    "outputId": "70263fd4-3c3a-4e93-a451-ee942131e0d4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model,hyperparams,train_set,valid_set,training_mode='Integrative_way'):\n",
    "    # train model_v1\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(51)\n",
    "    np.random.seed(51)\n",
    "    \n",
    "    # Instance callback\n",
    "    callbacks=myCallback()\n",
    "    \n",
    "    # Crerate train results folder\n",
    "    training_folder=fnn_model_v3.create_training_files(hyperparams=hyperparams)\n",
    "    \n",
    "    # register tensorboard writer\n",
    "    sensorboard_file=training_folder+'/tensorboard/'\n",
    "    if(os.path.exists(sensorboard_file)==False):\n",
    "        os.makedirs(sensorboard_file)\n",
    "    summary_writer=tf.summary.create_file_writer(sensorboard_file)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=hyperparams['learning_rate'], momentum=0.9)\n",
    "    \n",
    "    \"\"\" Integrated mode   \"\"\"\n",
    "    if training_mode=='Integrative_way':\n",
    "        model.compile(loss=tf.keras.losses.Huber(),\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=[\"mae\"])\n",
    "        history = model.fit(train_set,epochs=hyperparams['epochs'],validation_data=valid_set,callbacks=[callbacks])\n",
    "        history_dict=history.history\n",
    "    \"\"\" Specified mode   \"\"\"\n",
    "    if training_mode=='Manual_way':\n",
    "        tf.summary.trace_on(profiler=True) # 开启trace\n",
    "        for batch_idx, (X,y_true) in enumerate(train_set): \n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred=model(X)\n",
    "                loss=tf.reduce_mean(tf.square(y_pred-y_true))\n",
    "                # summary writer\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar('loss',loss,step=batch_idx)\n",
    "            # calculate grads\n",
    "            grads=tape.gradient(loss, model.variables)\n",
    "            # update params\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))\n",
    "        # summary trace\n",
    "        history_dict={\"loss\":'none'}\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.trace_export(name='model_trace',step=0,profiler_outdir=sensorboard_file)\n",
    "    \n",
    "    \n",
    "    # Save trained model and its parameters, history \n",
    "    save_trainedModel(model,history_dict,training_folder)\n",
    "    return model, history_dict, training_folder\n",
    "\n",
    "# Save trained model\n",
    "def save_trainedModel(trained_model,history_dict,training_folder,**args):\n",
    "    # Load hyperparameters \n",
    "    hyperparams_file=training_folder+\"/hyperparams.yaml\"\n",
    "    if os.path.isfile(hyperparams_file):\n",
    "        fr = open(hyperparams_file, 'r')\n",
    "        hyperparams = yaml.load(fr,Loader=yaml.BaseLoader)\n",
    "        fr.close()\n",
    "    # sub_idx of the subjects for training \n",
    "    train_sub_idx=hyperparams['train_sub_idx']\n",
    "    train_sub_idx_str=''\n",
    "    for ii in train_sub_idx:\n",
    "        train_sub_idx_str+='_'+str(ii)\n",
    "    # Save weights and models\n",
    "     #-----\n",
    "    \n",
    "    # checkpoints\n",
    "    checkpoint_folder=training_folder+'/checkpoints/'\n",
    "    if(os.path.exists(checkpoint_folder)==False):\n",
    "        os.makedirs(checkpoint_folder)\n",
    "    checkpoint_name='my_checkpoint_sub'+train_sub_idx_str+'.ckpt'\n",
    "    checkpoint=tf.train.Checkpoint(myAwesomeModel=model)\n",
    "    checkpoint_manager=tf.train.CheckpointManager(checkpoint,directory=checkpoint_folder,\n",
    "                                                  checkpoint_name=checkpoint_name,max_to_keep=20)\n",
    "    checkpoint_manager.save()\n",
    "    \n",
    "        \n",
    "    #saved_model\n",
    "    saved_model_folder=training_folder+'/saved_model/'\n",
    "    if(os.path.exists(saved_model_folder)==False):\n",
    "        os.makedirs(saved_model_folder)\n",
    "    saved_model_file=saved_model_folder+'my_model_sub'+train_sub_idx_str+'.h5'\n",
    "    trained_model.save(saved_model_file)\n",
    "    \n",
    "    # save history\n",
    "    import json\n",
    "    # Get the dictionary containing each metric and the loss for each epoch\n",
    "    history_path= training_folder+'/train_process/my_history_sub'+train_sub_idx_str\n",
    "    # Save it under the form of a json file\n",
    "    with open(history_path,'w') as fd:\n",
    "        json.dump(history_dict, fd)\n",
    "    # load history\n",
    "    #history_dict = json.load(open(history_path, 'r'))\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #trained_model,history, training_folder=train_model(model,hyperparams,train_set,valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Train model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_v2():\n",
    "    # train model_v2\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(51)\n",
    "    np.random.seed(51)\n",
    "    callbacks=myCallback()\n",
    "    history = model_v2.fit([xy_train_init,train_set],epochs=150),#validation_data=[valid_set_init,valid_set],callbacks=[callbacks])\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Plot training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history_dict):\n",
    "    print(history_dict.keys())\n",
    "    plt.plot(history_dict['loss'],'r')\n",
    "    plt.plot(history_dict['val_loss'],'g')\n",
    "    plt.plot(history_dict['mae'])\n",
    "    plt.legend(['train loss', 'valid loss','mae'])\n",
    "    \n",
    "    #plt.axis([0,150, 0.0,0.035])\n",
    "    print('max train MAE: {:.4f} and max val MAE: {:.4f}'.format(max(history_dict['mae']),max(history_dict['val_mae'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "id": "GaC6NNMRp0lb"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def test_model(training_folder, xy_test,scaler,**args):\n",
    "    \n",
    "    #1) Crerate test results folder\n",
    "    testing_folder=fnn_model_v3.create_testing_files(training_folder)\n",
    "    \n",
    "    #2) Load hyperparameters, Note the values in hyperparams become string type\n",
    "    hyperparams_file=training_folder+\"/hyperparams.yaml\"\n",
    "    if os.path.isfile(hyperparams_file):\n",
    "        fr = open(hyperparams_file, 'r')\n",
    "        hyperparams = yaml.load(fr,Loader=yaml.BaseLoader)\n",
    "        fr.close()\n",
    "\n",
    "    \n",
    "    #3) sub_idx of the subjects for training \n",
    "    train_sub_idx=hyperparams['train_sub_idx']\n",
    "    train_sub_idx_str=''\n",
    "    for ii in train_sub_idx:\n",
    "        train_sub_idx_str+='_'+str(ii)\n",
    "        \n",
    "    #4) Load model\n",
    "    saved_model_file=training_folder+'/saved_model/my_model_sub'+train_sub_idx_str+'.h5'\n",
    "    #saved_model_file=training_folder+'/saved_model/my_model_sub_'+'0123456789a'+'.h5'\n",
    "    #print(saved_model_file)\n",
    "    trained_model=tf.keras.models.load_model(saved_model_file)\n",
    "    \n",
    "    #5) Test data\n",
    "    model_output = model_forecast(trained_model, xy_test, \n",
    "                                  int(hyperparams['window_size']), int(hyperparams['batch_size']))\n",
    "    model_prediction=np.row_stack([model_output[:-1,0,:],model_output[-1,0:,:]])\n",
    "    \n",
    "    \n",
    "    #print(\"Test dataset shape:\",xy_test.shape)\n",
    "    #print(\"Model output shape\",model_output.shape)\n",
    "    #print(\"Model prediction shape\",model_prediction.shape)\n",
    "    \n",
    "    #6) Reshape and inverse normalization\n",
    "    prediction_xy_test=copy.deepcopy(xy_test) # deep copy of test data\n",
    "    prediction_xy_test[:,-6:]=model_prediction # using same shape with all datasets\n",
    "    predictions = scaler.inverse_transform(prediction_xy_test)[:,-6:] # inversed norm predition\n",
    "    labels  = scaler.inverse_transform(xy_test)[:,-6:]\n",
    "    features= scaler.inverse_transform(xy_test)[:,:-6]\n",
    "    \n",
    "    save_testResult(features,labels,predictions,testing_folder)\n",
    "    \n",
    "    return features,labels,predictions,testing_folder\n",
    "\n",
    "# save test results\n",
    "def save_testResult(features,labels,predictions,testing_folder):\n",
    "    saved_test_results_file=testing_folder+\"/test_results\"+'.h5'\n",
    "    with h5py.File(saved_test_results_file,'w') as fd:\n",
    "        fd.create_dataset('features',data=features)\n",
    "        fd.create_dataset('labels',data=labels)\n",
    "        fd.create_dataset('predictions',data=predictions)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #features, labels, predictions,testing_folder = test_model(training_folder,xy_test,scaler)\n",
    "    #print(\"labels shape\",labels.shape)\n",
    "    #print(\"features shape\",features.shape)\n",
    "    #print(\"Prediction shape\",predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plot results for paper\n",
    "## 6.1 Estimation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(features,labels,predictions,testing_folder,hyperparams):\n",
    "    \n",
    "    # Evaluate using two metrics, mae and mse\n",
    "    mae=tf.keras.metrics.mean_absolute_error(labels, predictions).numpy()\n",
    "    mse=tf.keras.metrics.mean_squared_error(labels, predictions).numpy()\n",
    "    print('MAE: {:.3f}, RMSE:{:.3f} \\n of six joint angles in 2.5 seconds'.format(np.mean(mae),np.mean(np.sqrt(mse))))\n",
    "    \n",
    "    \n",
    "    # hyper parameters    \n",
    "    features_names=hyperparams['features_names']\n",
    "    labels_names=hyperparams['labels_names']\n",
    "    \n",
    "    \n",
    "    # Save plot results\n",
    "    #sub_idx=15\n",
    "    test_sub_idx=hyperparams['test_sub_idx']\n",
    "    test_sub_idx_str=''\n",
    "    for ii in test_sub_idx:\n",
    "        test_sub_idx_str+='_'+str(ii)\n",
    "        \n",
    "    prediction_file=testing_folder+'/sub'+test_sub_idx_str+'_estimation.svg'\n",
    "    prediction_error_file=testing_folder+'/sub'+test_sub_idx_str+'_estimation_error.svg'\n",
    "    \n",
    "    \n",
    "    # Plot the estimation results and errors\n",
    "    fnn_model_v3.plot_test_results(features, labels, predictions, features_names, labels_names,testing_folder,\n",
    "                                   prediction_file=prediction_file,prediction_error_file=prediction_error_file)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #plot_prediction(features,labels,predictions,testing_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Statistical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "    \n",
    "def plot_prediction_statistic(features, labels, predictions):\n",
    "    # Plot the statistical results of the estimation results and errors\n",
    "    \n",
    "    error=abs(predictions-labels)\n",
    "    pd_error=pd.DataFrame(data=error,columns=labels_names)\n",
    "    NRMSE=100.0*np.sqrt(pd_error.apply(lambda x: x**2).mean(axis=0).to_frame().transpose())/(labels.max(axis=0)-labels.min(axis=0))\n",
    "    #*np.ones(pd_error.shape)*100\n",
    "    pd_NRMSE=pd.DataFrame(data=NRMSE, columns = [col for col in list(pd_error.columns)])\n",
    "    \n",
    "    \n",
    "    # create experiment results folder\n",
    "    # MAE\n",
    "    fig=plt.figure(figsize=(10,2))\n",
    "    style = ['darkgrid', 'dark', 'white', 'whitegrid', 'ticks']\n",
    "    sns.set_style(style[4],{'grid.color':'k'})\n",
    "    sns.catplot(data=pd_error,kind='bar', palette=\"Set3\").set(ylabel='Absolute error [deg]')\n",
    "    #plt.text(2.3,1.05, r\"$\\theta_{ae}(t)=abs(\\hat{\\theta}(t)-\\theta)(t)$\",horizontalalignment='center', fontsize=20)\n",
    "    savefig_file=basepath+'/sub_'+str(sub_idx)+'_mae.svg'\n",
    "    plt.savefig(savefig_file)\n",
    "    \n",
    "    \n",
    "    # NRMSE\n",
    "    fig=plt.figure(figsize=(10,3))\n",
    "    sns.catplot(data=pd_NRMSE,kind='bar', palette=\"Set3\").set(ylabel='NRMSE [%]')\n",
    "    #plt.text(2.3, 2.6, r\"$NRMSE=\\frac{\\sqrt{\\sum_{t=0}^{T}{\\theta^2_{ae}(t)}/T}}{\\theta_{max}-\\theta_{min}} \\times 100\\%$\",horizontalalignment='center', fontsize=20)\n",
    "    savefig_file=basepath+'/sub_'+str(sub_idx)+'_nrmse.svg'\n",
    "    plt.savefig(savefig_file)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function---1 Train and evaluation model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_idxs: [0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13]\n",
      "Raw data of subject ['sub_0', 'sub_1', 'sub_2', 'sub_3', 'sub_4', 'sub_5', 'sub_6', 'sub_7', 'sub_8', 'sub_9', 'sub_10', 'sub_11', 'sub_12', 'sub_13']\n",
      "Loaded dataset shape: (96983, 30)\n",
      "TRAIN: [ 1  2  3  4  5  6  7  8  9 10 11 12 13] TEST: [0]\n",
      "Train set shape (90032, 30)\n",
      "Valid set shape (6951, 30)\n",
      "Test set shape (6951, 30)\n",
      "Epoch 1/100\n",
      "2814/2814 [==============================] - 35s 11ms/step - loss: 0.0823 - mae: 0.2704 - val_loss: 0.2499 - val_mae: 0.5079\n",
      "Epoch 2/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0554 - mae: 0.2188 - val_loss: 0.2469 - val_mae: 0.4984\n",
      "Epoch 3/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0500 - mae: 0.2074 - val_loss: 0.2505 - val_mae: 0.5042\n",
      "Epoch 4/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0470 - mae: 0.2012 - val_loss: 0.2469 - val_mae: 0.5041\n",
      "Epoch 5/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0441 - mae: 0.1953 - val_loss: 0.2445 - val_mae: 0.4984\n",
      "Epoch 6/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0423 - mae: 0.1916 - val_loss: 0.2346 - val_mae: 0.4816\n",
      "Epoch 7/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0405 - mae: 0.1875 - val_loss: 0.2405 - val_mae: 0.4936\n",
      "Epoch 8/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0385 - mae: 0.1836 - val_loss: 0.2433 - val_mae: 0.5018\n",
      "Epoch 9/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0378 - mae: 0.1819 - val_loss: 0.2436 - val_mae: 0.5134\n",
      "Epoch 10/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0367 - mae: 0.1796 - val_loss: 0.2455 - val_mae: 0.5081\n",
      "Epoch 11/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0358 - mae: 0.1773 - val_loss: 0.2424 - val_mae: 0.5052\n",
      "Epoch 12/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0347 - mae: 0.1749 - val_loss: 0.2515 - val_mae: 0.5172\n",
      "Epoch 13/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0341 - mae: 0.1735 - val_loss: 0.2448 - val_mae: 0.5079\n",
      "Epoch 14/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0330 - mae: 0.1707 - val_loss: 0.2334 - val_mae: 0.4865\n",
      "Epoch 15/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0327 - mae: 0.1702 - val_loss: 0.2322 - val_mae: 0.4836\n",
      "Epoch 16/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0319 - mae: 0.1680 - val_loss: 0.2355 - val_mae: 0.4851\n",
      "Epoch 17/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0315 - mae: 0.1669 - val_loss: 0.2398 - val_mae: 0.5005\n",
      "Epoch 18/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0305 - mae: 0.1648 - val_loss: 0.2337 - val_mae: 0.4874\n",
      "Epoch 19/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0302 - mae: 0.1641 - val_loss: 0.2342 - val_mae: 0.4833\n",
      "Epoch 20/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0303 - mae: 0.1640 - val_loss: 0.2397 - val_mae: 0.4933\n",
      "Epoch 21/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0297 - mae: 0.1629 - val_loss: 0.2372 - val_mae: 0.4906\n",
      "Epoch 22/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0293 - mae: 0.1617 - val_loss: 0.2396 - val_mae: 0.4949\n",
      "Epoch 23/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0291 - mae: 0.1612 - val_loss: 0.2438 - val_mae: 0.4999\n",
      "Epoch 24/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0285 - mae: 0.1595 - val_loss: 0.2451 - val_mae: 0.5089\n",
      "Epoch 25/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0283 - mae: 0.1592 - val_loss: 0.2468 - val_mae: 0.5023\n",
      "Epoch 26/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0277 - mae: 0.1576 - val_loss: 0.2428 - val_mae: 0.4986\n",
      "Epoch 27/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0278 - mae: 0.1577 - val_loss: 0.2403 - val_mae: 0.4999\n",
      "Epoch 28/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0272 - mae: 0.1561 - val_loss: 0.2418 - val_mae: 0.4983\n",
      "Epoch 29/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0271 - mae: 0.1554 - val_loss: 0.2475 - val_mae: 0.5075\n",
      "Epoch 30/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0268 - mae: 0.1549 - val_loss: 0.2426 - val_mae: 0.5013\n",
      "Epoch 31/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0265 - mae: 0.1543 - val_loss: 0.2466 - val_mae: 0.5101\n",
      "Epoch 32/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0264 - mae: 0.1543 - val_loss: 0.2388 - val_mae: 0.4908\n",
      "Epoch 33/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0260 - mae: 0.1525 - val_loss: 0.2432 - val_mae: 0.5012\n",
      "Epoch 34/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0258 - mae: 0.1524 - val_loss: 0.2389 - val_mae: 0.4961\n",
      "Epoch 35/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0255 - mae: 0.1512 - val_loss: 0.2409 - val_mae: 0.5020\n",
      "Epoch 36/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0255 - mae: 0.1518 - val_loss: 0.2437 - val_mae: 0.5029\n",
      "Epoch 37/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0251 - mae: 0.1502 - val_loss: 0.2473 - val_mae: 0.5140\n",
      "Epoch 38/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0251 - mae: 0.1503 - val_loss: 0.2474 - val_mae: 0.5115\n",
      "Epoch 39/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0247 - mae: 0.1495 - val_loss: 0.2414 - val_mae: 0.5077\n",
      "Epoch 40/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0245 - mae: 0.1486 - val_loss: 0.2436 - val_mae: 0.5117\n",
      "Epoch 41/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0243 - mae: 0.1481 - val_loss: 0.2482 - val_mae: 0.5120\n",
      "Epoch 42/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0242 - mae: 0.1480 - val_loss: 0.2493 - val_mae: 0.5254\n",
      "Epoch 43/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0241 - mae: 0.1478 - val_loss: 0.2483 - val_mae: 0.5130\n",
      "Epoch 44/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0239 - mae: 0.1471 - val_loss: 0.2487 - val_mae: 0.5162\n",
      "Epoch 45/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0238 - mae: 0.1468 - val_loss: 0.2454 - val_mae: 0.5108\n",
      "Epoch 46/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0235 - mae: 0.1461 - val_loss: 0.2481 - val_mae: 0.5174\n",
      "Epoch 47/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0235 - mae: 0.1460 - val_loss: 0.2479 - val_mae: 0.5150\n",
      "Epoch 48/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0233 - mae: 0.1454 - val_loss: 0.2479 - val_mae: 0.5137\n",
      "Epoch 49/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0234 - mae: 0.1453 - val_loss: 0.2412 - val_mae: 0.5025\n",
      "Epoch 50/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0231 - mae: 0.1445 - val_loss: 0.2483 - val_mae: 0.5184\n",
      "Epoch 51/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0228 - mae: 0.1438 - val_loss: 0.2454 - val_mae: 0.5064\n",
      "Epoch 52/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0230 - mae: 0.1444 - val_loss: 0.2463 - val_mae: 0.5080\n",
      "Epoch 53/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0228 - mae: 0.1436 - val_loss: 0.2426 - val_mae: 0.5047\n",
      "Epoch 54/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0227 - mae: 0.1431 - val_loss: 0.2465 - val_mae: 0.5104\n",
      "Epoch 55/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0225 - mae: 0.1428 - val_loss: 0.2397 - val_mae: 0.4944\n",
      "Epoch 56/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0226 - mae: 0.1429 - val_loss: 0.2418 - val_mae: 0.5039\n",
      "Epoch 57/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0223 - mae: 0.1420 - val_loss: 0.2411 - val_mae: 0.4992\n",
      "Epoch 58/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0220 - mae: 0.1409 - val_loss: 0.2401 - val_mae: 0.4978\n",
      "Epoch 59/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0221 - mae: 0.1414 - val_loss: 0.2396 - val_mae: 0.4994\n",
      "Epoch 60/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0218 - mae: 0.1405 - val_loss: 0.2389 - val_mae: 0.4953\n",
      "Epoch 61/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0216 - mae: 0.1399 - val_loss: 0.2376 - val_mae: 0.4955\n",
      "Epoch 62/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0216 - mae: 0.1400 - val_loss: 0.2405 - val_mae: 0.4973\n",
      "Epoch 63/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0214 - mae: 0.1394 - val_loss: 0.2417 - val_mae: 0.4974\n",
      "Epoch 64/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0213 - mae: 0.1390 - val_loss: 0.2415 - val_mae: 0.5020\n",
      "Epoch 65/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0214 - mae: 0.1394 - val_loss: 0.2411 - val_mae: 0.4980\n",
      "Epoch 66/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0213 - mae: 0.1390 - val_loss: 0.2357 - val_mae: 0.4869\n",
      "Epoch 67/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0211 - mae: 0.1383 - val_loss: 0.2389 - val_mae: 0.5005\n",
      "Epoch 68/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0211 - mae: 0.1383 - val_loss: 0.2402 - val_mae: 0.4954\n",
      "Epoch 69/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0211 - mae: 0.1383 - val_loss: 0.2379 - val_mae: 0.4932\n",
      "Epoch 70/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0210 - mae: 0.1381 - val_loss: 0.2383 - val_mae: 0.4875\n",
      "Epoch 71/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0209 - mae: 0.1377 - val_loss: 0.2387 - val_mae: 0.4977\n",
      "Epoch 72/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0206 - mae: 0.1366 - val_loss: 0.2368 - val_mae: 0.4937\n",
      "Epoch 73/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0208 - mae: 0.1375 - val_loss: 0.2386 - val_mae: 0.4950\n",
      "Epoch 74/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0205 - mae: 0.1366 - val_loss: 0.2367 - val_mae: 0.4910\n",
      "Epoch 75/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0204 - mae: 0.1363 - val_loss: 0.2374 - val_mae: 0.4943\n",
      "Epoch 76/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0205 - mae: 0.1365 - val_loss: 0.2385 - val_mae: 0.4909\n",
      "Epoch 77/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0203 - mae: 0.1361 - val_loss: 0.2363 - val_mae: 0.4892\n",
      "Epoch 78/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0204 - mae: 0.1359 - val_loss: 0.2355 - val_mae: 0.4939\n",
      "Epoch 79/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0202 - mae: 0.1355 - val_loss: 0.2333 - val_mae: 0.4842\n",
      "Epoch 80/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0202 - mae: 0.1356 - val_loss: 0.2385 - val_mae: 0.4920\n",
      "Epoch 81/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0201 - mae: 0.1350 - val_loss: 0.2380 - val_mae: 0.4947\n",
      "Epoch 82/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0201 - mae: 0.1350 - val_loss: 0.2394 - val_mae: 0.4963\n",
      "Epoch 83/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0200 - mae: 0.1347 - val_loss: 0.2349 - val_mae: 0.4880\n",
      "Epoch 84/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0198 - mae: 0.1344 - val_loss: 0.2405 - val_mae: 0.4954\n",
      "Epoch 85/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0200 - mae: 0.1349 - val_loss: 0.2386 - val_mae: 0.4905\n",
      "Epoch 86/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0197 - mae: 0.1339 - val_loss: 0.2326 - val_mae: 0.4889\n",
      "Epoch 87/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0197 - mae: 0.1341 - val_loss: 0.2369 - val_mae: 0.4924\n",
      "Epoch 88/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0196 - mae: 0.1338 - val_loss: 0.2362 - val_mae: 0.4910\n",
      "Epoch 89/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0197 - mae: 0.1340 - val_loss: 0.2367 - val_mae: 0.4894\n",
      "Epoch 90/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0195 - mae: 0.1332 - val_loss: 0.2387 - val_mae: 0.4941\n",
      "Epoch 91/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0196 - mae: 0.1337 - val_loss: 0.2364 - val_mae: 0.4918\n",
      "Epoch 92/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0195 - mae: 0.1333 - val_loss: 0.2384 - val_mae: 0.4934\n",
      "Epoch 93/100\n",
      "2814/2814 [==============================] - 32s 11ms/step - loss: 0.0195 - mae: 0.1331 - val_loss: 0.2389 - val_mae: 0.4964\n",
      "Epoch 94/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0194 - mae: 0.1331 - val_loss: 0.2342 - val_mae: 0.4884\n",
      "Epoch 95/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0192 - mae: 0.1323 - val_loss: 0.2356 - val_mae: 0.4885\n",
      "Epoch 96/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0192 - mae: 0.1323 - val_loss: 0.2390 - val_mae: 0.4927\n",
      "Epoch 97/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0192 - mae: 0.1324 - val_loss: 0.2348 - val_mae: 0.4881\n",
      "Epoch 98/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0191 - mae: 0.1323 - val_loss: 0.2304 - val_mae: 0.4821\n",
      "Epoch 99/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0190 - mae: 0.1314 - val_loss: 0.2315 - val_mae: 0.4783\n",
      "Epoch 100/100\n",
      "2814/2814 [==============================] - 31s 11ms/step - loss: 0.0191 - mae: 0.1318 - val_loss: 0.2343 - val_mae: 0.4902\n",
      "TRAIN: [ 0  2  3  4  5  6  7  8  9 10 11 12 13] TEST: [1]\n",
      "Train set shape (89544, 30)\n",
      "Valid set shape (7439, 30)\n",
      "Test set shape (7439, 30)\n",
      "Epoch 1/100\n",
      "2798/2798 [==============================] - 36s 11ms/step - loss: 0.1072 - mae: 0.3035 - val_loss: 0.1701 - val_mae: 0.3993\n",
      "Epoch 2/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0671 - mae: 0.2392 - val_loss: 0.1645 - val_mae: 0.3966\n",
      "Epoch 3/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0602 - mae: 0.2268 - val_loss: 0.1609 - val_mae: 0.3913\n",
      "Epoch 4/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0555 - mae: 0.2184 - val_loss: 0.1575 - val_mae: 0.3872\n",
      "Epoch 5/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0520 - mae: 0.2107 - val_loss: 0.1491 - val_mae: 0.3783\n",
      "Epoch 6/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0489 - mae: 0.2044 - val_loss: 0.1527 - val_mae: 0.3755\n",
      "Epoch 7/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0467 - mae: 0.1998 - val_loss: 0.1602 - val_mae: 0.3898\n",
      "Epoch 8/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0453 - mae: 0.1974 - val_loss: 0.1614 - val_mae: 0.3878\n",
      "Epoch 9/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0436 - mae: 0.1934 - val_loss: 0.1654 - val_mae: 0.3963\n",
      "Epoch 10/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0430 - mae: 0.1929 - val_loss: 0.1677 - val_mae: 0.3994\n",
      "Epoch 11/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0415 - mae: 0.1896 - val_loss: 0.1694 - val_mae: 0.4027\n",
      "Epoch 12/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0403 - mae: 0.1871 - val_loss: 0.1667 - val_mae: 0.4035\n",
      "Epoch 13/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0397 - mae: 0.1856 - val_loss: 0.1580 - val_mae: 0.3870\n",
      "Epoch 14/100\n",
      "2798/2798 [==============================] - 32s 11ms/step - loss: 0.0389 - mae: 0.1841 - val_loss: 0.1725 - val_mae: 0.4126\n",
      "Epoch 15/100\n",
      "2798/2798 [==============================] - 32s 11ms/step - loss: 0.0384 - mae: 0.1831 - val_loss: 0.1712 - val_mae: 0.4084\n",
      "Epoch 16/100\n",
      "2798/2798 [==============================] - 32s 11ms/step - loss: 0.0370 - mae: 0.1800 - val_loss: 0.1734 - val_mae: 0.4064\n",
      "Epoch 17/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0365 - mae: 0.1790 - val_loss: 0.1724 - val_mae: 0.4059\n",
      "Epoch 18/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0360 - mae: 0.1781 - val_loss: 0.1725 - val_mae: 0.4112\n",
      "Epoch 19/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0351 - mae: 0.1757 - val_loss: 0.1728 - val_mae: 0.4094\n",
      "Epoch 20/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0346 - mae: 0.1751 - val_loss: 0.1717 - val_mae: 0.4106\n",
      "Epoch 21/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0345 - mae: 0.1741 - val_loss: 0.1762 - val_mae: 0.4202\n",
      "Epoch 22/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0337 - mae: 0.1728 - val_loss: 0.1695 - val_mae: 0.4082\n",
      "Epoch 23/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0334 - mae: 0.1719 - val_loss: 0.1655 - val_mae: 0.4018\n",
      "Epoch 24/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0330 - mae: 0.1709 - val_loss: 0.1612 - val_mae: 0.3983\n",
      "Epoch 25/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0321 - mae: 0.1689 - val_loss: 0.1676 - val_mae: 0.4102\n",
      "Epoch 26/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0317 - mae: 0.1679 - val_loss: 0.1687 - val_mae: 0.4058\n",
      "Epoch 27/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0317 - mae: 0.1676 - val_loss: 0.1693 - val_mae: 0.4079\n",
      "Epoch 28/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0315 - mae: 0.1674 - val_loss: 0.1612 - val_mae: 0.3968\n",
      "Epoch 29/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0307 - mae: 0.1652 - val_loss: 0.1737 - val_mae: 0.4140\n",
      "Epoch 30/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0302 - mae: 0.1640 - val_loss: 0.1691 - val_mae: 0.4089\n",
      "Epoch 31/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0301 - mae: 0.1636 - val_loss: 0.1650 - val_mae: 0.4029\n",
      "Epoch 32/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0300 - mae: 0.1633 - val_loss: 0.1714 - val_mae: 0.4196\n",
      "Epoch 33/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0293 - mae: 0.1614 - val_loss: 0.1723 - val_mae: 0.4171\n",
      "Epoch 34/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0289 - mae: 0.1609 - val_loss: 0.1703 - val_mae: 0.4158\n",
      "Epoch 35/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0287 - mae: 0.1600 - val_loss: 0.1724 - val_mae: 0.4172\n",
      "Epoch 36/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0284 - mae: 0.1592 - val_loss: 0.1671 - val_mae: 0.4104\n",
      "Epoch 37/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0285 - mae: 0.1598 - val_loss: 0.1751 - val_mae: 0.4217\n",
      "Epoch 38/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0281 - mae: 0.1582 - val_loss: 0.1712 - val_mae: 0.4143\n",
      "Epoch 39/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0275 - mae: 0.1569 - val_loss: 0.1670 - val_mae: 0.4074\n",
      "Epoch 40/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0274 - mae: 0.1567 - val_loss: 0.1676 - val_mae: 0.4069\n",
      "Epoch 41/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0274 - mae: 0.1568 - val_loss: 0.1697 - val_mae: 0.4075\n",
      "Epoch 42/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0270 - mae: 0.1558 - val_loss: 0.1665 - val_mae: 0.4031\n",
      "Epoch 43/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0269 - mae: 0.1554 - val_loss: 0.1632 - val_mae: 0.4000\n",
      "Epoch 44/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0267 - mae: 0.1549 - val_loss: 0.1676 - val_mae: 0.4065\n",
      "Epoch 45/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0267 - mae: 0.1549 - val_loss: 0.1669 - val_mae: 0.4051\n",
      "Epoch 46/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0267 - mae: 0.1552 - val_loss: 0.1684 - val_mae: 0.4056\n",
      "Epoch 47/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0262 - mae: 0.1540 - val_loss: 0.1638 - val_mae: 0.4013\n",
      "Epoch 48/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0261 - mae: 0.1535 - val_loss: 0.1643 - val_mae: 0.3992\n",
      "Epoch 49/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0258 - mae: 0.1525 - val_loss: 0.1663 - val_mae: 0.4055\n",
      "Epoch 50/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0258 - mae: 0.1524 - val_loss: 0.1684 - val_mae: 0.4084\n",
      "Epoch 51/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0255 - mae: 0.1517 - val_loss: 0.1649 - val_mae: 0.4069\n",
      "Epoch 52/100\n",
      "2798/2798 [==============================] - 32s 11ms/step - loss: 0.0256 - mae: 0.1520 - val_loss: 0.1717 - val_mae: 0.4091\n",
      "Epoch 53/100\n",
      "2798/2798 [==============================] - 32s 11ms/step - loss: 0.0253 - mae: 0.1512 - val_loss: 0.1581 - val_mae: 0.3930\n",
      "Epoch 54/100\n",
      "2798/2798 [==============================] - 32s 11ms/step - loss: 0.0253 - mae: 0.1511 - val_loss: 0.1631 - val_mae: 0.4026\n",
      "Epoch 55/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0251 - mae: 0.1505 - val_loss: 0.1663 - val_mae: 0.4061\n",
      "Epoch 56/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0248 - mae: 0.1497 - val_loss: 0.1654 - val_mae: 0.4041\n",
      "Epoch 57/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0246 - mae: 0.1486 - val_loss: 0.1710 - val_mae: 0.4139\n",
      "Epoch 58/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0245 - mae: 0.1490 - val_loss: 0.1647 - val_mae: 0.4041\n",
      "Epoch 59/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0245 - mae: 0.1486 - val_loss: 0.1636 - val_mae: 0.4065\n",
      "Epoch 60/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0241 - mae: 0.1477 - val_loss: 0.1612 - val_mae: 0.3984\n",
      "Epoch 61/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0243 - mae: 0.1479 - val_loss: 0.1614 - val_mae: 0.3990\n",
      "Epoch 62/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0240 - mae: 0.1470 - val_loss: 0.1568 - val_mae: 0.3904\n",
      "Epoch 63/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0242 - mae: 0.1475 - val_loss: 0.1586 - val_mae: 0.3999\n",
      "Epoch 64/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0240 - mae: 0.1470 - val_loss: 0.1642 - val_mae: 0.4052\n",
      "Epoch 65/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0239 - mae: 0.1468 - val_loss: 0.1602 - val_mae: 0.3957\n",
      "Epoch 66/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0238 - mae: 0.1465 - val_loss: 0.1590 - val_mae: 0.3937\n",
      "Epoch 67/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0234 - mae: 0.1453 - val_loss: 0.1604 - val_mae: 0.3944\n",
      "Epoch 68/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0235 - mae: 0.1456 - val_loss: 0.1586 - val_mae: 0.3891\n",
      "Epoch 69/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0234 - mae: 0.1452 - val_loss: 0.1526 - val_mae: 0.3851\n",
      "Epoch 70/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0230 - mae: 0.1439 - val_loss: 0.1639 - val_mae: 0.4041\n",
      "Epoch 71/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0233 - mae: 0.1450 - val_loss: 0.1584 - val_mae: 0.3966\n",
      "Epoch 72/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0229 - mae: 0.1436 - val_loss: 0.1598 - val_mae: 0.3986\n",
      "Epoch 73/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0232 - mae: 0.1445 - val_loss: 0.1591 - val_mae: 0.3965\n",
      "Epoch 74/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0228 - mae: 0.1435 - val_loss: 0.1549 - val_mae: 0.3902\n",
      "Epoch 75/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0230 - mae: 0.1440 - val_loss: 0.1595 - val_mae: 0.3980\n",
      "Epoch 76/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0229 - mae: 0.1439 - val_loss: 0.1584 - val_mae: 0.3982\n",
      "Epoch 77/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0225 - mae: 0.1427 - val_loss: 0.1574 - val_mae: 0.3936\n",
      "Epoch 78/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0226 - mae: 0.1427 - val_loss: 0.1600 - val_mae: 0.3989\n",
      "Epoch 79/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0223 - mae: 0.1421 - val_loss: 0.1588 - val_mae: 0.3960\n",
      "Epoch 80/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0224 - mae: 0.1421 - val_loss: 0.1557 - val_mae: 0.3957\n",
      "Epoch 81/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0225 - mae: 0.1422 - val_loss: 0.1608 - val_mae: 0.4024\n",
      "Epoch 82/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0224 - mae: 0.1421 - val_loss: 0.1547 - val_mae: 0.3917\n",
      "Epoch 83/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0222 - mae: 0.1415 - val_loss: 0.1582 - val_mae: 0.3980\n",
      "Epoch 84/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0221 - mae: 0.1409 - val_loss: 0.1545 - val_mae: 0.3903\n",
      "Epoch 85/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0220 - mae: 0.1409 - val_loss: 0.1565 - val_mae: 0.3919\n",
      "Epoch 86/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0221 - mae: 0.1411 - val_loss: 0.1572 - val_mae: 0.3983\n",
      "Epoch 87/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0220 - mae: 0.1406 - val_loss: 0.1544 - val_mae: 0.3910\n",
      "Epoch 88/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0218 - mae: 0.1400 - val_loss: 0.1576 - val_mae: 0.3962\n",
      "Epoch 89/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0216 - mae: 0.1394 - val_loss: 0.1557 - val_mae: 0.3952\n",
      "Epoch 90/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0217 - mae: 0.1399 - val_loss: 0.1536 - val_mae: 0.3922\n",
      "Epoch 91/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0216 - mae: 0.1394 - val_loss: 0.1567 - val_mae: 0.3963\n",
      "Epoch 92/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0216 - mae: 0.1398 - val_loss: 0.1573 - val_mae: 0.3950\n",
      "Epoch 93/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0219 - mae: 0.1403 - val_loss: 0.1570 - val_mae: 0.3962\n",
      "Epoch 94/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0216 - mae: 0.1394 - val_loss: 0.1572 - val_mae: 0.3957\n",
      "Epoch 95/100\n",
      "2798/2798 [==============================] - 32s 11ms/step - loss: 0.0215 - mae: 0.1389 - val_loss: 0.1565 - val_mae: 0.3935\n",
      "Epoch 96/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0215 - mae: 0.1390 - val_loss: 0.1520 - val_mae: 0.3893\n",
      "Epoch 97/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0214 - mae: 0.1389 - val_loss: 0.1590 - val_mae: 0.4002\n",
      "Epoch 98/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0214 - mae: 0.1388 - val_loss: 0.1564 - val_mae: 0.3965\n",
      "Epoch 99/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0212 - mae: 0.1384 - val_loss: 0.1530 - val_mae: 0.3889\n",
      "Epoch 100/100\n",
      "2798/2798 [==============================] - 31s 11ms/step - loss: 0.0211 - mae: 0.1379 - val_loss: 0.1559 - val_mae: 0.3935\n",
      "TRAIN: [ 0  1  3  4  5  6  7  8  9 10 11 12 13] TEST: [2]\n",
      "Train set shape (89297, 30)\n",
      "Valid set shape (7686, 30)\n",
      "Test set shape (7686, 30)\n",
      "Epoch 1/100\n",
      "2791/2791 [==============================] - 36s 11ms/step - loss: 0.0996 - mae: 0.2925 - val_loss: 0.3313 - val_mae: 0.6016\n",
      "Epoch 2/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0603 - mae: 0.2283 - val_loss: 0.3185 - val_mae: 0.5836\n",
      "Epoch 3/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0526 - mae: 0.2138 - val_loss: 0.3060 - val_mae: 0.5714\n",
      "Epoch 4/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0487 - mae: 0.2059 - val_loss: 0.3263 - val_mae: 0.5875\n",
      "Epoch 5/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0462 - mae: 0.2006 - val_loss: 0.3189 - val_mae: 0.5862\n",
      "Epoch 6/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0447 - mae: 0.1978 - val_loss: 0.3080 - val_mae: 0.5685\n",
      "Epoch 7/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0423 - mae: 0.1925 - val_loss: 0.3138 - val_mae: 0.5847\n",
      "Epoch 8/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0417 - mae: 0.1911 - val_loss: 0.3202 - val_mae: 0.5890\n",
      "Epoch 9/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0398 - mae: 0.1867 - val_loss: 0.3031 - val_mae: 0.5673\n",
      "Epoch 10/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0388 - mae: 0.1845 - val_loss: 0.3098 - val_mae: 0.5817\n",
      "Epoch 11/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0374 - mae: 0.1811 - val_loss: 0.3025 - val_mae: 0.5703\n",
      "Epoch 12/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0367 - mae: 0.1800 - val_loss: 0.2975 - val_mae: 0.5570\n",
      "Epoch 13/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0361 - mae: 0.1783 - val_loss: 0.3085 - val_mae: 0.5756\n",
      "Epoch 14/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0350 - mae: 0.1752 - val_loss: 0.3077 - val_mae: 0.5713\n",
      "Epoch 15/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0346 - mae: 0.1745 - val_loss: 0.3028 - val_mae: 0.5676\n",
      "Epoch 16/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0338 - mae: 0.1731 - val_loss: 0.3057 - val_mae: 0.5708\n",
      "Epoch 17/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0329 - mae: 0.1705 - val_loss: 0.3129 - val_mae: 0.5774\n",
      "Epoch 18/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0327 - mae: 0.1699 - val_loss: 0.3118 - val_mae: 0.5729\n",
      "Epoch 19/100\n",
      "2791/2791 [==============================] - 31s 11ms/step - loss: 0.0319 - mae: 0.1684 - val_loss: 0.3021 - val_mae: 0.5659\n",
      "Epoch 20/100\n",
      "1828/2791 [==================>...........] - ETA: 10s - loss: 0.0318 - mae: 0.1672"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-33-63b9f667c1b3>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     68\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     69\u001b[0m     \u001b[0;31m#3) train model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 70\u001b[0;31m     \u001b[0mtrained_model\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhistory_dict\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtraining_folder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     71\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     72\u001b[0m     \u001b[0;31m#4) test model\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-27-6589876ed1bb>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, hyperparams, train_set, valid_set, training_mode)\u001b[0m\n\u001b[1;32m     24\u001b[0m                       \u001b[0moptimizer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     25\u001b[0m                       metrics=[\"mae\"])\n\u001b[0;32m---> 26\u001b[0;31m         \u001b[0mhistory\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'epochs'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mvalid_set\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mcallbacks\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     27\u001b[0m         \u001b[0mhistory_dict\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     28\u001b[0m     \u001b[0;34m\"\"\" Specified mode   \"\"\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/tensorflow/python/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, validation_batch_size, validation_freq, max_queue_size, workers, use_multiprocessing)\u001b[0m\n\u001b[1;32m   1098\u001b[0m                 _r=1):\n\u001b[1;32m   1099\u001b[0m               \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_train_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1100\u001b[0;31m               \u001b[0mtmp_logs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain_function\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miterator\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1101\u001b[0m               \u001b[0;32mif\u001b[0m \u001b[0mdata_handler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshould_sync\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1102\u001b[0m                 \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masync_wait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    826\u001b[0m     \u001b[0mtracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    827\u001b[0m     \u001b[0;32mwith\u001b[0m \u001b[0mtrace\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTrace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_name\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtm\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 828\u001b[0;31m       \u001b[0mresult\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    829\u001b[0m       \u001b[0mcompiler\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"xla\"\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_experimental_compile\u001b[0m \u001b[0;32melse\u001b[0m \u001b[0;34m\"nonXla\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    830\u001b[0m       \u001b[0mnew_tracing_count\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexperimental_get_tracing_count\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/tensorflow/python/eager/def_function.py\u001b[0m in \u001b[0;36m_call\u001b[0;34m(self, *args, **kwds)\u001b[0m\n\u001b[1;32m    853\u001b[0m       \u001b[0;31m# In this case we have created variables on the first call, so we run the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    854\u001b[0m       \u001b[0;31m# defunned version which is guaranteed to never create variables.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 855\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateless_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# pylint: disable=not-callable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    856\u001b[0m     \u001b[0;32melif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_stateful_fn\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    857\u001b[0m       \u001b[0;31m# Release the lock early so that multiple threads can perform the call\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   2940\u001b[0m       (graph_function,\n\u001b[1;32m   2941\u001b[0m        filtered_flat_args) = self._maybe_define_function(args, kwargs)\n\u001b[0;32m-> 2942\u001b[0;31m     return graph_function._call_flat(\n\u001b[0m\u001b[1;32m   2943\u001b[0m         filtered_flat_args, captured_inputs=graph_function.captured_inputs)  # pylint: disable=protected-access\n\u001b[1;32m   2944\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36m_call_flat\u001b[0;34m(self, args, captured_inputs, cancellation_manager)\u001b[0m\n\u001b[1;32m   1916\u001b[0m         and executing_eagerly):\n\u001b[1;32m   1917\u001b[0m       \u001b[0;31m# No tape is watching; skip to running the function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1918\u001b[0;31m       return self._build_call_outputs(self._inference_function.call(\n\u001b[0m\u001b[1;32m   1919\u001b[0m           ctx, args, cancellation_manager=cancellation_manager))\n\u001b[1;32m   1920\u001b[0m     forward_backward = self._select_forward_and_backward_functions(\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/tensorflow/python/eager/function.py\u001b[0m in \u001b[0;36mcall\u001b[0;34m(self, ctx, args, cancellation_manager)\u001b[0m\n\u001b[1;32m    553\u001b[0m       \u001b[0;32mwith\u001b[0m \u001b[0m_InterpolateFunctionError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    554\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mcancellation_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 555\u001b[0;31m           outputs = execute.execute(\n\u001b[0m\u001b[1;32m    556\u001b[0m               \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msignature\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mname\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    557\u001b[0m               \u001b[0mnum_outputs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_outputs\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/miniconda3/envs/myconda/lib/python3.8/site-packages/tensorflow/python/eager/execute.py\u001b[0m in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     57\u001b[0m   \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0mctx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mensure_initialized\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 59\u001b[0;31m     tensors = pywrap_tfe.TFE_Py_Execute(ctx._handle, device_name, op_name,\n\u001b[0m\u001b[1;32m     60\u001b[0m                                         inputs, attrs, num_outputs)\n\u001b[1;32m     61\u001b[0m   \u001b[0;32mexcept\u001b[0m \u001b[0mcore\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_NotOkStatusException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import time as localtimepkg\n",
    "\n",
    "\n",
    "\n",
    "# Setup hyperparameters\n",
    "hyperparams=initParameters()\n",
    "\n",
    "# Setup devices, using gpu\n",
    "cpus=tf.config.list_physical_devices(device_type='CPU')\n",
    "gpus=tf.config.list_physical_devices(device_type='GPU')\n",
    "print(cpus,gpus)\n",
    "tf.config.set_visible_devices(devices=gpus[0:2],device_type='GPU')\n",
    "\n",
    "# normalize all subject data\n",
    "\n",
    "def normalize_subjects_data(sub_idxs):\n",
    "    assert(isinstance(sub_idxs,list))\n",
    "    xy_data, scaler = load_normalize_data(['sub_'+str(ii) for ii in sub_idxs])\n",
    "    subject_data_len=fnn_model_v3.all_datasets_len\n",
    "    \n",
    "    norm_subs_data={}\n",
    "    start,end=0,0\n",
    "    for ii, sub_idx in enumerate(sub_idxs):\n",
    "        sub_idx_str='sub_'+str(sub_idx)\n",
    "        if(ii==0):\n",
    "            start=0\n",
    "        else:\n",
    "            start=end\n",
    "        end+=subject_data_len[sub_idx_str]\n",
    "        \n",
    "        norm_subs_data[sub_idx_str]=xy_data[start:end,:]\n",
    "    return norm_subs_data, scaler\n",
    "\n",
    "\n",
    "# A list of training and testing files\n",
    "train_test_folder= \"./models_parameters_results/\"+str(localtimepkg.strftime(\"%Y-%m-%d\", localtimepkg.localtime()))\n",
    "if(os.path.exists(train_test_folder)==False):\n",
    "    os.makedirs(train_test_folder)    \n",
    "train_test_folder_log=train_test_folder+\"/train_test_folder.log\"\n",
    "if(os.path.exists(train_test_folder_log)):\n",
    "    os.remove(train_test_folder_log)\n",
    "log_dict={'training_folder':[],'testing_folder':[]}\n",
    "\n",
    "# load and normalize datasets for training and testing\n",
    "norm_subs_data, scaler=normalize_subjects_data(hyperparams['sub_idx'])\n",
    "# leave-one-out cross-validation\n",
    "loo = LeaveOneOut()\n",
    "for train_index, test_index in loo.split(hyperparams['sub_idx']):\n",
    "    \n",
    "    # train and test subject dataset \n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    hyperparams['train_sub_idx']=[str(ii) for ii in  train_index] # the values of params should be str or int types\n",
    "    hyperparams['test_sub_idx']=[str(ii) for ii in test_index]\n",
    "    xy_train=[norm_subs_data['sub_'+str(ii)] for ii in train_index]\n",
    "    xy_test=[norm_subs_data['sub_'+str(ii)] for ii in test_index]\n",
    "    \n",
    "    xy_train=np.concatenate(xy_train,axis=0)\n",
    "    xy_test=np.concatenate(xy_test,axis=0)\n",
    "    xy_valid=xy_test\n",
    "    \n",
    "    #1) load dataset\n",
    "    train_set = windowed_dataset(xy_train, hyperparams['window_size'], hyperparams['batch_size'], shuffle_buffer=1000)\n",
    "    valid_set = windowed_dataset(xy_valid, hyperparams['window_size'], hyperparams['batch_size'], shuffle_buffer=1000)\n",
    "    print(\"Train set shape\",xy_train.shape)\n",
    "    print(\"Valid set shape\",xy_valid.shape)\n",
    "    print(\"Test set shape\",xy_test.shape)\n",
    "    #print(\"X Shape for a iteration train\",list(train_set.as_numpy_iterator())[0][0].shape)\n",
    "    #print(\"Y Shape for a iteration train\",list(train_set.as_numpy_iterator())[0][1].shape)\n",
    "\n",
    "    #2) declare model\n",
    "    model=model_v1(hyperparams)\n",
    "    \n",
    "    #3) train model\n",
    "    trained_model,history_dict,training_folder=train_model(model,hyperparams,train_set,valid_set)\n",
    "    \n",
    "    #4) test model\n",
    "    features, labels, predictions, testing_folder = test_model(training_folder,xy_test,scaler)\n",
    "    log_dict['training_folder'].append(training_folder)\n",
    "    log_dict['testing_folder'].append(testing_folder)\n",
    "     \n",
    "    #5) Plot\n",
    "    #plot_prediction(features,labels,predictions,testing_folder,hyperparams)\n",
    "    #plot_prediction_statistic(features, labels, predictions)\n",
    "\n",
    "\n",
    "#6) save train and test folder path\n",
    "with open(train_test_folder_log,'w') as fd:\n",
    "    yaml.dump(log_dict,fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tf' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-9de3e24f24d4>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mcpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_logical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'CPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mgpus\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconfig\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlist_logical_devices\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdevice_type\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'GPU'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcpus\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mgpus\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mhyperparams\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tf' is not defined"
     ]
    }
   ],
   "source": [
    "cpus=tf.config.list_logical_devices(device_type='CPU')\n",
    "gpus=tf.config.list_logical_devices(device_type='GPU')\n",
    "print(cpus,gpus)\n",
    "\n",
    "print(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "history_file='/home/sun/Downloads/my_history_sub_0_1_2_3_5'\n",
    "with open(history_file,'r') as fd:\n",
    "    his=json.load(fd)\n",
    "    \n",
    "plot_history(history_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function---2 Display training and testing results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.4.0\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'torch'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-3-aea073146c15>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"tensorflow version:\"\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__version__\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;31m# load datasets in a numpy\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mfnn_model_v3\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mfnn_model_v3\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0m__name__\u001b[0m\u001b[0;34m==\u001b[0m\u001b[0;34m'__main__'\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/mnt/fnn_model_v3.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \"\"\"\n\u001b[0;32m---> 12\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     13\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnn\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0mtorch\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mutils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdata\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mDataset\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDataLoader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mTensorDataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'torch'"
     ]
    }
   ],
   "source": [
    "## import necessary packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import os\n",
    "import yaml\n",
    "import h5py\n",
    "print(\"tensorflow version:\",tf.__version__)\n",
    "# load datasets in a numpy \n",
    "import fnn_model_v3 as fnn_model_v3\n",
    "\n",
    "if __name__=='__main__':\n",
    "    #1) test model\n",
    "    testing_folder='/models_parameters_results/2021-08-11/test_061028/'\n",
    "    training_folder='/models_parameters_results/2021-08-11/training_061028/'\n",
    "    if(testing_folder==None):\n",
    "        features, labels, predictions, testing_folder = test_model(training_folder,xy_test,scaler)\n",
    "        log_dict['training_folder'].append(training_folder)\n",
    "        log_dict['testing_folder'].append(testing_folder)\n",
    "    else:\n",
    "        testing_results=testing_folder+'test_1/test_results.hs'\n",
    "        with h5py.File(testing_results,'r') as fd:\n",
    "            features=fd['features']\n",
    "            predictions=fd['predictions']\n",
    "            labels=fd['labels']\n",
    "\n",
    "    \n",
    "    plot_prediction(features,labels,predictions,testing_folder,hyperparams)\n",
    "    plot_prediction_statistic(features, labels, predictions)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“S+P Week 4 Exercise Answer.ipynb”的副本",
   "provenance": [
    {
     "file_id": "https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%204%20-%20S%2BP/S%2BP%20Week%204%20Exercise%20Answer.ipynb",
     "timestamp": 1626510374525
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
