{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zX4Kg8DUTKWO"
   },
   "source": [
    "# <centr>Description of this project </center>\n",
    "## There are 14 subjects attending the experiments, they are indexed by following keys:\n",
    "#['sub_0','sub_1', 'sub_2', 'sub_3', 'sub_4', 'sub_5', 'sub_6', 'sub_7', 'sub_8', 'sub_9'，'sub_10', 'sub_11', 'sub_12', 'sub_13',sub_13]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "'''\n",
    "all_datasets_len={'sub_0':6951, 'sub_1':7439, 'sub_2': 7686, 'sub_3': 8678, 'sub_4':6180, 'sub_5': 6671, \n",
    "                   'sub_6': 7600, 'sub_7': 5583, 'sub_8': 6032, 'sub_9': 6508, 'sub_10': 6348, 'sub_11': 7010, 'sub_12': 8049, 'sub_13': 6248}\n",
    "all_datasets_ranges={'sub_-1':0, sub_0': 6951, 'sub_1': 14390, 'sub_2': 22076, 'sub_3': 30754, 'sub_4': 36934, 'sub_5': 43605,\n",
    "                      'sub_6': 51205, 'sub_7': 56788, 'sub_8': 62820, 'sub_9': 69328, 'sub_10': 75676, 'sub_11':82686, 'sub_12': 90735, 'sub_13': 96983}\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Hyper parametes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensorflow version: 2.5.0\n"
     ]
    }
   ],
   "source": [
    "## import necessary packages\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pdb\n",
    "import os\n",
    "import yaml\n",
    "import h5py\n",
    "print(\"tensorflow version:\",tf.__version__)\n",
    "# load datasets in a numpy \n",
    "import fnn_model_v3 as fnn_model_v3\n",
    "\n",
    "def initParameters():\n",
    "    # hyper parameters\n",
    "    hyperparams=fnn_model_v3.hyperparams\n",
    "    \n",
    "    labels_names=[ 'L_IE', 'L_AA', 'L_FE','R_IE', 'R_AA', 'R_FE' ]\n",
    "    \n",
    "    '''\n",
    "    features_names=[\n",
    "        'L_Up_Acc_X', 'L_Up_Acc_Y', 'L_Up_Acc_Z', 'L_Up_Gyr_X', 'L_Up_Gyr_Y','L_Up_Gyr_Z',\n",
    "        'L_Up_Mag_X','L_Up_Mag_Y','L_Up_Mag_Z',\n",
    "        \n",
    "        'L_Lower_Acc_X', 'L_Lower_Acc_Y', 'L_Lower_Acc_Z', 'L_Lower_Gyr_X', 'L_Lower_Gyr_Y','L_Lower_Gyr_Z',\n",
    "        'L_Lower_Mag_X',   'L_Lower_Mag_Y','L_Lower_Mag_Z',\n",
    "        \n",
    "        'R_Up_Acc_X', 'R_Up_Acc_Y', 'R_Up_Acc_Z', 'R_Up_Gyr_X', 'R_Up_Gyr_Y','R_Up_Gyr_Z', \n",
    "        'R_Up_Mag_X', 'R_Up_Mag_Y', 'R_Up_Mag_Z',\n",
    "        \n",
    "        'R_Lower_Acc_X', 'R_Lower_Acc_Y', 'R_Lower_Acc_Z', 'R_Lower_Gyr_X', 'R_Lower_Gyr_Y','R_Lower_Gyr_Z', \n",
    "        'R_Lower_Mag_X',   'R_Lower_Mag_Y','R_Lower_Mag_Z'\n",
    "        ]    \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    # without magnetometer data\n",
    "    features_names=[\n",
    "        'L_Up_Acc_X', 'L_Up_Acc_Y', 'L_Up_Acc_Z', 'L_Up_Gyr_X', 'L_Up_Gyr_Y','L_Up_Gyr_Z', \n",
    "        \n",
    "        'L_Lower_Acc_X', 'L_Lower_Acc_Y', 'L_Lower_Acc_Z', 'L_Lower_Gyr_X', 'L_Lower_Gyr_Y','L_Lower_Gyr_Z', \n",
    "        \n",
    "        'R_Up_Acc_X', 'R_Up_Acc_Y', 'R_Up_Acc_Z', 'R_Up_Gyr_X', 'R_Up_Gyr_Y','R_Up_Gyr_Z', \n",
    "        \n",
    "        'R_Lower_Acc_X', 'R_Lower_Acc_Y', 'R_Lower_Acc_Z', 'R_Lower_Gyr_X', 'R_Lower_Gyr_Y','R_Lower_Gyr_Z'\n",
    "        ]    \n",
    "    \n",
    "    sub_idx=[0,1,2,3,4,5,6,7,8,9,10,11,12,13]\n",
    "    #sub_idx=[0,1]\n",
    "    \n",
    "    columns_names=features_names+labels_names\n",
    "    hyperparams['features_num']=len(features_names)\n",
    "    hyperparams['labels_num']=len(labels_names)\n",
    "    hyperparams['features_names']=features_names;\n",
    "    hyperparams['labels_names']=labels_names\n",
    "    hyperparams['learning_rate']=10e-2\n",
    "    hyperparams['batch_size']=32\n",
    "    hyperparams['window_size']=10\n",
    "    hyperparams['epochs']=100\n",
    "    hyperparams['columns_names']=columns_names\n",
    "    hyperparams['raw_dataset_path']= './datasets_files/raw_datasets.hdf5'\n",
    "    hyperparams['sub_idx']=sub_idx\n",
    "    \n",
    "    print('sub_idxs:',sub_idx)\n",
    "    return hyperparams\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #hyperparams=initParameters()\n",
    "    #print(\"window size:\", hyperparams['window_size'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Prepare dataset for training and evaluation\n",
    "## 2.1 Read raw subject data and normalization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "def load_normalize_data(sub_idx,scaler=None):\n",
    "    \n",
    "    #**** Single subject test\n",
    "    if(isinstance(sub_idx,int)):\n",
    "        start=fnn_model_v3.all_datasets_ranges['sub_'+str(sub_idx-1)]\n",
    "        end=fnn_model_v3.all_datasets_ranges['sub_'+str(sub_idx)]\n",
    "        series=fnn_model_v3.read_rawdata(range(start,end),hyperparams['columns_names'],hyperparams['raw_dataset_path'])\n",
    "        print(\"Raw data of subject {:}, rows from {:} to {:}\".format(sub_idx,start,end))\n",
    "    \n",
    "    #**** Multiple subject data indexed by numbers\n",
    "    if(isinstance(sub_idx,list) and isinstance(sub_idx[0],int)):\n",
    "        start_sub_num=int(sub_idx[0])\n",
    "        end_sub_num=int(sub_idx[-1])\n",
    "        start=fnn_model_v3.all_datasets_ranges['sub_'+str(start_sub_num-1)]\n",
    "        end=fnn_model_v3.all_datasets_ranges['sub_'+str(end_sub_num)]\n",
    "        series=fnn_model_v3.read_rawdata(range(start,end),hyperparams['columns_names'],hyperparams['raw_dataset_path'])\n",
    "        print(\"Raw data of subject {:}, rows from {:} to {:}\".format(sub_idx,start,end))\n",
    "    \n",
    "    #**** Multiple subject data indexed by \"sub_num\"\n",
    "    if(isinstance(sub_idx,list) and isinstance(sub_idx[0],str)):\n",
    "        series_temp=[]\n",
    "        for idx in sub_idx:\n",
    "            assert(isinstance(idx,str))\n",
    "            series_temp.append(fnn_model_v3.read_rawdata(idx,hyperparams['columns_names'],hyperparams['raw_dataset_path']))\n",
    "        series=np.concatenate(series_temp,axis=0)\n",
    "        print(\"Raw data of subject {:}\".format(sub_idx))\n",
    "    # load dataset\n",
    "    print('Loaded dataset shape:',series.shape)\n",
    "    \n",
    "    #Normalization data\n",
    "    if scaler==None:\n",
    "        scaler=StandardScaler()\n",
    "    scaler.fit(series)\n",
    "    scaled_series=scaler.transform(series.astype(np.float32),copy=True)\n",
    "    \n",
    "    # Get the initial stage data of every subjects\n",
    "    init_stage_sub_ranges={keys:range(fnn_model_v3.all_datasets_ranges['sub_'+str(int(keys[4:])-1)],fnn_model_v3.all_datasets_ranges['sub_'+str(int(keys[4:])-1)]+300)\n",
    "                  for keys in ['sub_'+str(idx) for idx in range(14)]}\n",
    "    init_stage_sub_data={keys:fnn_model_v3.read_rawdata(values,hyperparams['columns_names'],hyperparams['raw_dataset_path']) for keys, values in init_stage_sub_ranges.items()}\n",
    "    # Normalization of init-stage data\n",
    "    scaled_init_stage_sub_data={keys:scaler.transform(values).mean(axis=0,keepdims=True) \n",
    "                                for keys, values in init_stage_sub_data.items()}\n",
    "    #empirically_orientation=[-2.32, 2.77,-3.5,  -8.65, 4.0, 3.01, -0.22, -2.11, 5.29, -9.47, -2.85, -1.33]\n",
    "    return scaled_series, scaler\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #scaled_series,scaler=load_normalize_data(sub_idx=['sub_1','sub_2','sub_4'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.2 Split datasets into train, valid and test data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def split_dataset(scaled_series,sub_idx):\n",
    "    #1) Split raw dataset into train, valid and test dataset\n",
    "    if(isinstance(sub_idx,int)):# split single subject data \n",
    "        all_train_split={'sub_0':5600,'sub_1':6000,'sub_2':6400,'sub_3':7500,\n",
    "                         'sub_4':4900,'sub_5':6000,'sub_6':4500,'sub_7':4500,\n",
    "                         'sub_8':4900,'sub_9':5100,'sub_10':5000,'sub_11':5700,'sub_12':6400,'sub_13':4000}\n",
    "        all_valid_split={'sub_0':6500,'sub_1':6800,'sub_2':7000,'sub_3':8100,\n",
    "                         'sub_4':5600,'sub_5':6900,'sub_6':5100,'sub_7':5000,\n",
    "                         'sub_8':5600,'sub_9':5900,'sub_10':5800,'sub_11':6200,'sub_12':7200, 'sub_13':4900}\n",
    "        train_split = all_train_split['sub_'+str(sub_idx)]\n",
    "        valid_split= all_valid_split['sub_'+str(sub_idx)]\n",
    "    if(isinstance(sub_idx,list)):# split multiple subject data, using leave-one-out cross-validtion\n",
    "        train_split=scaled_series.shape[0]-3000\n",
    "        valid_split=scaled_series.shape[0]-2000\n",
    "    \n",
    "    xy_train = scaled_series[:train_split,:]\n",
    "    xy_valid = scaled_series[train_split:valid_split,:]\n",
    "    xy_test = scaled_series[valid_split:,:]\n",
    "    shuffle_buffer_size = 6000\n",
    "    \n",
    "    \n",
    "    # Sensor segment calibration transfer process\n",
    "    Init_stage_calibration=False\n",
    "    if(Init_stage_calibration):\n",
    "        np.random.seed(101)\n",
    "        transfer_weight=np.random.randn(features_num,features_num)\n",
    "        transfer_temp=np.dot(scaled_init_stage_sub_data['sub_'+str(sub_idx)][:,:features_num],transfer_weight)\n",
    "        xy_train[:,:features_num]=xy_train[:,:features_num]+np.tanh(transfer_temp)\n",
    "        xy_valid[:,:features_num]=xy_valid[:,:features_num]+np.tanh(transfer_temp)\n",
    "        xy_test[:,:features_num]=xy_test[:,:features_num]+np.tanh(transfer_temp)\n",
    "        # init info fom calibration stage \n",
    "        xy_train_init=scaled_init_stage_sub_data['sub_'+str(sub_idx)]*np.ones(xy_train.shape)\n",
    "        xy_valid_init=scaled_init_stage_sub_data['sub_'+str(sub_idx)]*np.ones(xy_valid.shape)\n",
    "        xy_test_init=scaled_init_stage_sub_data['sub_'+str(sub_idx)]*np.ones(xy_test.shape)\n",
    "    \n",
    "    print(\"Subject {:} dataset\".format(sub_idx))\n",
    "    print(\"xy_train shape:\",xy_train.shape)\n",
    "    print(\"xy valid shape:\",xy_valid.shape)\n",
    "    print(\"xy_test shape:\",xy_test.shape)\n",
    "    \n",
    "    return xy_train, xy_valid,xy_test\n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #xy_train, xy_valid, xy_test = split_dataset(scaled_series,sub_idx=[1,2])\n",
    "    #hyperparams['train_sub_idx']=[1,2,4]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2.3 Transfer data into dataset for feeding to neural network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "id": "lJwUUZscnG38"
   },
   "outputs": [],
   "source": [
    "\n",
    "# packing data into windows \n",
    "def windowed_dataset(series, window_size, batch_size, shuffle_buffer):\n",
    "    #series = tf.expand_dims(series, axis=-1)\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series)\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.shuffle(shuffle_buffer)\n",
    "    ds = ds.map(lambda w: (w[:,:-6], w[:,-6:]))\n",
    "    ds=ds.batch(batch_size).prefetch(1)\n",
    "    return ds\n",
    "\n",
    "# model prediction\n",
    "def model_forecast(model, series, window_size, batch_size):\n",
    "    ds = tf.data.Dataset.from_tensor_slices(series[:,:-6])\n",
    "    ds = ds.window(window_size, shift=1, drop_remainder=True)\n",
    "    ds = ds.flat_map(lambda w: w.batch(window_size))\n",
    "    ds = ds.batch(batch_size).prefetch(1)\n",
    "    #print(list(ds.as_numpy_iterator())[0])\n",
    "    # model_prediction\n",
    "    forecast = model.predict(ds)\n",
    "    return forecast\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    # transfer the data into dataset for feeding to neural network\n",
    "    pass\n",
    "    #train_set = windowed_dataset(xy_train, hyperparams['window_size'], hyperparams['batch_size'], shuffle_buffer=1000)\n",
    "    #valid_set = windowed_dataset(xy_valid, hyperparams['window_size'], hyperparams['batch_size'], shuffle_buffer=1000)\n",
    "    #train_set_init = tf.data.Dataset.from_tensor_slices(xy_train_init).batch(1).map(lambda x: (x[:,:-6],x[:,-6:]))\n",
    "    #valid_set_init = tf.data.Dataset.from_tensor_slices(xy_valid_init).batch(1).map(lambda x: (x[:,:-6],x[:,-6:]))\n",
    "    #test_set_init = windowed_dataset(xy_test_init,window_size,batch_size,shuffle_buffer_size)\n",
    "\n",
    "    \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# display datasets\n",
    "\n",
    "#datasets_ranges=(fnn_model_v3.all_datasets_ranges['sub_'+str(sub_idx-1)],fnn_model_v3.all_datasets_ranges['sub_'+str(sub_idx)])\n",
    "#fnn_model_v3.display_rawdatase(series[31000:34000,:], columns_names, norm_type=None, raw_datasets_path=raw_dataset_path,plot_title='sub_'+str(sub_idx))\n",
    "#fnn_model_v3.display_rawdatase(scaled_series[6000:6250,:], columns_names, norm_type=None, raw_datasets_path=raw_dataset_path,plot_title='sub_'+str(sub_idx))\n",
    "#fnn_model_v3.display_rawdatase(scaler.inverse_transform(scaled_series[6000:6250,:]), columns_names, norm_type=None, raw_datasets_path=raw_dataset_path)\n",
    "#fnn_model_v3.display_rawdatase(datasets_ranges, columns_names, norm_type='mean_std', raw_datasets_path=raw_dataset_path,plot_title='raw data')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Model definition\n",
    "## 3.1 Model v1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model definne\n",
    "def model_v1(hyperparams):\n",
    "    model = tf.keras.models.Sequential([\n",
    "      tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
    "                          strides=1, padding=\"causal\",\n",
    "                          activation=\"relu\",\n",
    "                          input_shape=[None, hyperparams['features_num']]),\n",
    "      tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "      tf.keras.layers.LSTM(60, return_sequences=True),\n",
    "      #tf.keras.layers.Flatten(),\n",
    "      tf.keras.layers.Dense(60, activation=\"relu\"),\n",
    "      tf.keras.layers.Dense(30),\n",
    "      tf.keras.layers.Dense(6)\n",
    "      #tf.keras.layers.Lambda(lambda x: x *180)\n",
    "    ])\n",
    "    return model\n",
    "\n",
    "# Callback class\n",
    "class myCallback(tf.keras.callbacks.Callback):\n",
    "    def on_epoch_end(self, epcoh, logs={}):\n",
    "        if(logs.get('loss')<0.003):\n",
    "            print('\\nLoss is low so cancelling training!')\n",
    "            self.model.stop_training = True\n",
    "\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #model=model_v1(hyperparams)\n",
    "    #print(model.summary())\n",
    "#training_folder=fnn_model_v3.create_training_files(hyperparams=fnn_model_v3.hyperparams)\n",
    "#model summary\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3.2 Model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义组合神经网络，多个子网络\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras import Model\n",
    "\n",
    "def model_v2():\n",
    "    print(tf.__version__)\n",
    "    ####定义一个方便构造常规 Sequential() 网络的函数\n",
    "    def DNN_A_Graph(x,n_input=36,n_output=6,name='Transfer_graph'):\n",
    "        tf.random.set_seed(50)\n",
    "        np.random.seed(50)\n",
    "        he = tf.initializers.he_normal()\n",
    "        elu = tf.nn.elu\n",
    "        x=Dense(n_input, kernel_initializer=he, activation=elu,name=name+'_1')(x)\n",
    "        x=Dense(n_output,kernel_initializer=he, activation=elu,name=name+'_2')(x)\n",
    "        return(x)\n",
    "    \n",
    "    def DNN_B_Graph(x,n_input=36,n_output=6,name='Main_graph'):\n",
    "        ##\n",
    "        x=tf.keras.layers.Conv1D(filters=60, kernel_size=5,\n",
    "                          strides=1, padding=\"causal\",\n",
    "                          activation=\"relu\",\n",
    "                          input_shape=[window_size, features_num],name='Conv1D')(x)\n",
    "        x=tf.keras.layers.LSTM(60, return_sequences=True,name='lstm_1')(x)\n",
    "        x=tf.keras.layers.LSTM(60, return_sequences=True,name='lstm_2')(x)\n",
    "        x=tf.keras.layers.Flatten()(x)\n",
    "        x=tf.keras.layers.Dense(60, activation=\"relu\")(x)\n",
    "        x=tf.keras.layers.Dense(30)(x)\n",
    "        x=tf.keras.layers.Dense(6)(x)\n",
    "        return(x)\n",
    "        \n",
    "        \n",
    "    #### 构造并联网络图\n",
    "    ##需要并联的两个网络的输入\n",
    "    input_a=tf.keras.layers.Input(shape=[features_num],name='Input_A')\n",
    "    input_b=tf.keras.layers.Input(shape=[hyperparams['window_size'],features_num],name='Input_B')\n",
    "    window_size=hyperparams['window_size']\n",
    "    ##构造两个需要并联的子网络结构\n",
    "    dnn_a=DNN_A_Graph(input_a,n_input=features_num,n_output=labels_num,name=\"DNN_A\")\n",
    "    dnn_b=DNN_B_Graph(input_b,n_input=features_num,n_output=labels_num,name=\"DNN_B\")\n",
    "    ##concat操作\n",
    "    concat=tf.keras.layers.concatenate([dnn_a,dnn_b],axis=-1,name=\"Concat_Layer\")\n",
    "    ##在concat基础上继续添加一些层\n",
    "    output=Dense(labels_num,name=\"Output_Layer\")(concat)\n",
    "    ##这一步很关键：这一步相当于把输入和输出对应起来，形成系统认识的一个完整的图。\n",
    "    model_v2=Model(inputs=[input_a,input_b],outputs=[output])\n",
    "    model_v2.get_layer('DNN_A_1').trainable=False\n",
    "    model_v2.get_layer('DNN_A_2').trainable=False\n",
    "    \n",
    "    ##网络的其他组件\n",
    "    optimizer=tf.keras.optimizers.Adam()\n",
    "    loss_fn=tf.keras.losses.mean_squared_error\n",
    "    model_v2.compile(loss=loss_fn,\n",
    "                  optimizer=optimizer,\n",
    "                  metrics=['accuracy'],\n",
    "                  )\n",
    "    \n",
    "    model_v2.summary()\n",
    "    #### 训练和测试：这里的x1,x2,对应前述的input_a和input_b\n",
    "    #model.fit(x=[x1,x2],y=y,epochs=10,batch_size=500,verbose=2)\n",
    "    \n",
    "    #model.evaluate(x=[x1_test,x2_test],y=y_test,verbose=2)\n",
    "    return model_v2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "AclfYY3Mn6Ph",
    "outputId": "dd1fef93-d819-4d56-df20-330169907e16",
    "tags": []
   },
   "source": [
    "# find optimal lr\n",
    "\n",
    "tf.keras.backend.clear_session()\n",
    "tf.random.set_seed(51)\n",
    "np.random.seed(51)\n",
    "\n",
    "lr_schedule = tf.keras.callbacks.LearningRateScheduler(\n",
    "    lambda epoch: 1e-8 * 10**(epoch / 20))\n",
    "optimizer = tf.keras.optimizers.SGD(learning_rate=1e-8, momentum=0.9)\n",
    "model.compile(loss=tf.keras.losses.Huber(),\n",
    "              optimizer=optimizer,\n",
    "              metrics=[\"mae\"])\n",
    "history = model.fit(train_set, epochs=150, callbacks=[lr_schedule])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 290
    },
    "id": "vVcKmg7Q_7rD",
    "outputId": "5e9b8029-e996-4a2b-e016-666c69865b11"
   },
   "source": [
    "plt.semilogx(history.history[\"lr\"], history.history[\"loss\"])\n",
    "plt.axis([1e-8, 1, 0, 2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "plt.plot(history.history['loss'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 4. Train model\n",
    "## 4.1 Train model V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "QsksvkcXAAgq",
    "outputId": "70263fd4-3c3a-4e93-a451-ee942131e0d4"
   },
   "outputs": [],
   "source": [
    "\n",
    "def train_model(model,hyperparams,train_set,valid_set,training_mode='Integrative_way'):\n",
    "    # train model_v1\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(51)\n",
    "    np.random.seed(51)\n",
    "    \n",
    "    # Instance callback\n",
    "    callbacks=myCallback()\n",
    "    \n",
    "    # Crerate train results folder\n",
    "    training_folder=fnn_model_v3.create_training_files(hyperparams=hyperparams)\n",
    "    \n",
    "    # register tensorboard writer\n",
    "    sensorboard_file=training_folder+'/tensorboard/'\n",
    "    if(os.path.exists(sensorboard_file)==False):\n",
    "        os.makedirs(sensorboard_file)\n",
    "    summary_writer=tf.summary.create_file_writer(sensorboard_file)\n",
    "    \n",
    "    optimizer = tf.keras.optimizers.SGD(learning_rate=hyperparams['learning_rate'], momentum=0.9)\n",
    "    \n",
    "    \"\"\" Integrated mode   \"\"\"\n",
    "    if training_mode=='Integrative_way':\n",
    "        model.compile(loss=tf.keras.losses.Huber(),\n",
    "                      optimizer=optimizer,\n",
    "                      metrics=[\"mae\"])\n",
    "        history = model.fit(train_set,epochs=hyperparams['epochs'],validation_data=valid_set,callbacks=[callbacks])\n",
    "        history_dict=history.history\n",
    "    \"\"\" Specified mode   \"\"\"\n",
    "    if training_mode=='Manual_way':\n",
    "        tf.summary.trace_on(profiler=True) # 开启trace\n",
    "        for batch_idx, (X,y_true) in enumerate(train_set): \n",
    "            with tf.GradientTape() as tape:\n",
    "                y_pred=model(X)\n",
    "                loss=tf.reduce_mean(tf.square(y_pred-y_true))\n",
    "                # summary writer\n",
    "                with summary_writer.as_default():\n",
    "                    tf.summary.scalar('loss',loss,step=batch_idx)\n",
    "            # calculate grads\n",
    "            grads=tape.gradient(loss, model.variables)\n",
    "            # update params\n",
    "            optimizer.apply_gradients(grads_and_vars=zip(grads,model.variables))\n",
    "        # summary trace\n",
    "        history_dict={\"loss\":'none'}\n",
    "        with summary_writer.as_default():\n",
    "            tf.summary.trace_export(name='model_trace',step=0,profiler_outdir=sensorboard_file)\n",
    "    \n",
    "    \n",
    "    # Save trained model and its parameters, history \n",
    "    save_trainedModel(model,history_dict,training_folder)\n",
    "    return model, history_dict, training_folder\n",
    "\n",
    "# Save trained model\n",
    "def save_trainedModel(trained_model,history_dict,training_folder,**args):\n",
    "    # Load hyperparameters \n",
    "    hyperparams_file=training_folder+\"/hyperparams.yaml\"\n",
    "    if os.path.isfile(hyperparams_file):\n",
    "        fr = open(hyperparams_file, 'r')\n",
    "        hyperparams = yaml.load(fr,Loader=yaml.BaseLoader)\n",
    "        fr.close()\n",
    "    # sub_idx of the subjects for training \n",
    "    train_sub_idx=hyperparams['train_sub_idx']\n",
    "    train_sub_idx_str=''\n",
    "    for ii in train_sub_idx:\n",
    "        train_sub_idx_str+='_'+str(ii)\n",
    "    # Save weights and models\n",
    "     #-----\n",
    "    \n",
    "    # checkpoints\n",
    "    checkpoint_folder=training_folder+'/checkpoints/'\n",
    "    if(os.path.exists(checkpoint_folder)==False):\n",
    "        os.makedirs(checkpoint_folder)\n",
    "    checkpoint_name='my_checkpoint_sub'+train_sub_idx_str+'.ckpt'\n",
    "    checkpoint=tf.train.Checkpoint(myAwesomeModel=model)\n",
    "    checkpoint_manager=tf.train.CheckpointManager(checkpoint,directory=checkpoint_folder,\n",
    "                                                  checkpoint_name=checkpoint_name,max_to_keep=20)\n",
    "    checkpoint_manager.save()\n",
    "    \n",
    "        \n",
    "    #saved_model\n",
    "    saved_model_folder=training_folder+'/saved_model/'\n",
    "    if(os.path.exists(saved_model_folder)==False):\n",
    "        os.makedirs(saved_model_folder)\n",
    "    saved_model_file=saved_model_folder+'my_model_sub'+train_sub_idx_str+'.h5'\n",
    "    trained_model.save(saved_model_file)\n",
    "    \n",
    "    # save history\n",
    "    import json\n",
    "    # Get the dictionary containing each metric and the loss for each epoch\n",
    "    history_path= training_folder+'/train_process/my_history_sub'+train_sub_idx_str\n",
    "    # Save it under the form of a json file\n",
    "    with open(history_path,'w') as fd:\n",
    "        json.dump(history_dict, fd)\n",
    "    # load history\n",
    "    #history_dict = json.load(open(history_path, 'r'))\n",
    "    \n",
    "    \n",
    "    \n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #trained_model,history, training_folder=train_model(model,hyperparams,train_set,valid_set)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.2 Train model V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_model_v2():\n",
    "    # train model_v2\n",
    "    tf.keras.backend.clear_session()\n",
    "    tf.random.set_seed(51)\n",
    "    np.random.seed(51)\n",
    "    callbacks=myCallback()\n",
    "    history = model_v2.fit([xy_train_init,train_set],epochs=150),#validation_data=[valid_set_init,valid_set],callbacks=[callbacks])\n",
    "    return history\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4.3 Plot training process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history_dict):\n",
    "    print(history_dict.keys())\n",
    "    plt.plot(history_dict['loss'],'r')\n",
    "    plt.plot(history_dict['val_loss'],'g')\n",
    "    plt.plot(history_dict['mae'])\n",
    "    plt.legend(['train loss', 'valid loss','mae'])\n",
    "    \n",
    "    #plt.axis([0,150, 0.0,0.035])\n",
    "    print('max train MAE: {:.4f} and max val MAE: {:.4f}'.format(max(history_dict['mae']),max(history_dict['val_mae'])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5. Test model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "id": "GaC6NNMRp0lb"
   },
   "outputs": [],
   "source": [
    "import copy\n",
    "def test_model(training_folder, xy_test,scaler,**args):\n",
    "    \n",
    "    #1) Crerate test results folder\n",
    "    testing_folder=fnn_model_v3.create_testing_files(training_folder)\n",
    "    \n",
    "    #2) Load hyperparameters, Note the values in hyperparams become string type\n",
    "    hyperparams_file=training_folder+\"/hyperparams.yaml\"\n",
    "    if os.path.isfile(hyperparams_file):\n",
    "        fr = open(hyperparams_file, 'r')\n",
    "        hyperparams = yaml.load(fr,Loader=yaml.BaseLoader)\n",
    "        fr.close()\n",
    "\n",
    "    \n",
    "    #3) sub_idx of the subjects for training \n",
    "    train_sub_idx=hyperparams['train_sub_idx']\n",
    "    train_sub_idx_str=''\n",
    "    for ii in train_sub_idx:\n",
    "        train_sub_idx_str+='_'+str(ii)\n",
    "        \n",
    "    #4) Load model\n",
    "    saved_model_file=training_folder+'/saved_model/my_model_sub'+train_sub_idx_str+'.h5'\n",
    "    #saved_model_file=training_folder+'/saved_model/my_model_sub_'+'0123456789a'+'.h5'\n",
    "    #print(saved_model_file)\n",
    "    trained_model=tf.keras.models.load_model(saved_model_file)\n",
    "    \n",
    "    #5) Test data\n",
    "    model_output = model_forecast(trained_model, xy_test, \n",
    "                                  int(hyperparams['window_size']), int(hyperparams['batch_size']))\n",
    "    model_prediction=np.row_stack([model_output[:-1,0,:],model_output[-1,0:,:]])\n",
    "    \n",
    "    \n",
    "    #print(\"Test dataset shape:\",xy_test.shape)\n",
    "    #print(\"Model output shape\",model_output.shape)\n",
    "    #print(\"Model prediction shape\",model_prediction.shape)\n",
    "    \n",
    "    #6) Reshape and inverse normalization\n",
    "    prediction_xy_test=copy.deepcopy(xy_test) # deep copy of test data\n",
    "    prediction_xy_test[:,-6:]=model_prediction # using same shape with all datasets\n",
    "    predictions = scaler.inverse_transform(prediction_xy_test)[:,-6:] # inversed norm predition\n",
    "    labels  = scaler.inverse_transform(xy_test)[:,-6:]\n",
    "    features= scaler.inverse_transform(xy_test)[:,:-6]\n",
    "    \n",
    "    save_testResult(features,labels,predictions,testing_folder)\n",
    "    \n",
    "    return features,labels,predictions,testing_folder\n",
    "\n",
    "# save test results\n",
    "def save_testResult(features,labels,predictions,testing_folder):\n",
    "    saved_test_results_file=testing_folder+\"/test_results\"+'.h5'\n",
    "    with h5py.File(saved_test_results_file,'w') as fd:\n",
    "        fd.create_dataset('features',data=features)\n",
    "        fd.create_dataset('labels',data=labels)\n",
    "        fd.create_dataset('predictions',data=predictions)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #features, labels, predictions,testing_folder = test_model(training_folder,xy_test,scaler)\n",
    "    #print(\"labels shape\",labels.shape)\n",
    "    #print(\"features shape\",features.shape)\n",
    "    #print(\"Prediction shape\",predictions.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 6. Plot results for paper\n",
    "## 6.1 Estimation accuracy "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_prediction(features,labels,predictions,testing_folder,hyperparams):\n",
    "    \n",
    "    # Evaluate using two metrics, mae and mse\n",
    "    mae=tf.keras.metrics.mean_absolute_error(labels, predictions).numpy()\n",
    "    mse=tf.keras.metrics.mean_squared_error(labels, predictions).numpy()\n",
    "    print('MAE: {:.3f}, RMSE:{:.3f} \\n of six joint angles in 2.5 seconds'.format(np.mean(mae),np.mean(np.sqrt(mse))))\n",
    "    \n",
    "    \n",
    "    # hyper parameters    \n",
    "    features_names=hyperparams['features_names']\n",
    "    labels_names=hyperparams['labels_names']\n",
    "    \n",
    "    \n",
    "    # Save plot results\n",
    "    #sub_idx=15\n",
    "    test_sub_idx=hyperparams['test_sub_idx']\n",
    "    test_sub_idx_str=''\n",
    "    for ii in test_sub_idx:\n",
    "        test_sub_idx_str+='_'+str(ii)\n",
    "        \n",
    "    prediction_file=testing_folder+'/sub'+test_sub_idx_str+'_estimation.svg'\n",
    "    prediction_error_file=testing_folder+'/sub'+test_sub_idx_str+'_estimation_error.svg'\n",
    "    \n",
    "    \n",
    "    # Plot the estimation results and errors\n",
    "    fnn_model_v3.plot_test_results(features, labels, predictions, features_names, labels_names,testing_folder,\n",
    "                                   prediction_file=prediction_file,prediction_error_file=prediction_error_file)\n",
    "\n",
    "if __name__=='__main__':\n",
    "    pass\n",
    "    #plot_prediction(features,labels,predictions,testing_folder)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6.2 Statistical plots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "    \n",
    "def plot_prediction_statistic(features, labels, predictions):\n",
    "    # Plot the statistical results of the estimation results and errors\n",
    "    \n",
    "    error=abs(predictions-labels)\n",
    "    pd_error=pd.DataFrame(data=error,columns=labels_names)\n",
    "    NRMSE=100.0*np.sqrt(pd_error.apply(lambda x: x**2).mean(axis=0).to_frame().transpose())/(labels.max(axis=0)-labels.min(axis=0))\n",
    "    #*np.ones(pd_error.shape)*100\n",
    "    pd_NRMSE=pd.DataFrame(data=NRMSE, columns = [col for col in list(pd_error.columns)])\n",
    "    \n",
    "    \n",
    "    # create experiment results folder\n",
    "    # MAE\n",
    "    fig=plt.figure(figsize=(10,2))\n",
    "    style = ['darkgrid', 'dark', 'white', 'whitegrid', 'ticks']\n",
    "    sns.set_style(style[4],{'grid.color':'k'})\n",
    "    sns.catplot(data=pd_error,kind='bar', palette=\"Set3\").set(ylabel='Absolute error [deg]')\n",
    "    #plt.text(2.3,1.05, r\"$\\theta_{ae}(t)=abs(\\hat{\\theta}(t)-\\theta)(t)$\",horizontalalignment='center', fontsize=20)\n",
    "    savefig_file=basepath+'/sub_'+str(sub_idx)+'_mae.svg'\n",
    "    plt.savefig(savefig_file)\n",
    "    \n",
    "    \n",
    "    # NRMSE\n",
    "    fig=plt.figure(figsize=(10,3))\n",
    "    sns.catplot(data=pd_NRMSE,kind='bar', palette=\"Set3\").set(ylabel='NRMSE [%]')\n",
    "    #plt.text(2.3, 2.6, r\"$NRMSE=\\frac{\\sqrt{\\sum_{t=0}^{T}{\\theta^2_{ae}(t)}/T}}{\\theta_{max}-\\theta_{min}} \\times 100\\%$\",horizontalalignment='center', fontsize=20)\n",
    "    savefig_file=basepath+'/sub_'+str(sub_idx)+'_nrmse.svg'\n",
    "    plt.savefig(savefig_file)\n",
    "    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Main function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sub_idxs: [0, 1]\n",
      "Raw data of subject ['sub_0', 'sub_1']\n",
      "Loaded dataset shape: (14390, 30)\n",
      "TRAIN: [1] TEST: [0]\n",
      "Train set shape (7439, 30)\n",
      "Valid set shape (6951, 30)\n",
      "Test set shape (6951, 30)\n",
      "233/233 [==============================] - 5s 11ms/step - loss: 0.1129 - mae: 0.3282 - val_loss: 0.2835 - val_mae: 0.5565\n",
      "TRAIN: [0] TEST: [1]\n",
      "Train set shape (6951, 30)\n",
      "Valid set shape (7439, 30)\n",
      "Test set shape (7439, 30)\n",
      "217/217 [==============================] - 5s 12ms/step - loss: 0.2001 - mae: 0.4427 - val_loss: 0.2129 - val_mae: 0.4817\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.model_selection import LeaveOneOut\n",
    "import time as localtimepkg\n",
    "\n",
    "\n",
    "\n",
    "# Setup hyperparameters\n",
    "hyperparams=initParameters()\n",
    "\n",
    "# normalize all subject data\n",
    "\n",
    "def normalize_subjects_data(sub_idxs):\n",
    "    assert(isinstance(sub_idxs,list))\n",
    "    xy_data, scaler = load_normalize_data(['sub_'+str(ii) for ii in sub_idxs])\n",
    "    subject_data_len=fnn_model_v3.all_datasets_len\n",
    "    \n",
    "    norm_subs_data={}\n",
    "    start,end=0,0\n",
    "    for ii, sub_idx in enumerate(sub_idxs):\n",
    "        sub_idx_str='sub_'+str(sub_idx)\n",
    "        if(ii==0):\n",
    "            start=0\n",
    "        else:\n",
    "            start=end\n",
    "        end+=subject_data_len[sub_idx_str]\n",
    "        \n",
    "        norm_subs_data[sub_idx_str]=xy_data[start:end,:]\n",
    "    return norm_subs_data, scaler\n",
    "\n",
    "\n",
    "# A list of training and testing files\n",
    "train_test_folder= \"./models_parameters_results/\"+str(localtimepkg.strftime(\"%Y-%m-%d\", localtimepkg.localtime()))\n",
    "if(os.path.exists(train_test_folder)==False):\n",
    "    os.makedirs(train_test_folder)    \n",
    "train_test_folder_log=train_test_folder+\"/train_test_folder.log\"\n",
    "if(os.path.exists(train_test_folder_log)):\n",
    "    os.remove(train_test_folder_log)\n",
    "log_dict={'training_folder':[],'testing_folder':[]}\n",
    "\n",
    "# load and normalize datasets for training and testing\n",
    "norm_subs_data, scaler=normalize_subjects_data(hyperparams['sub_idx'])\n",
    "# leave-one-out cross-validation\n",
    "loo = LeaveOneOut()\n",
    "for train_index, test_index in loo.split(hyperparams['sub_idx']):\n",
    "    \n",
    "    # train and test subject dataset \n",
    "    print(\"TRAIN:\", train_index, \"TEST:\", test_index)\n",
    "    hyperparams['train_sub_idx']=[str(ii) for ii in  train_index] # the values of params should be str or int types\n",
    "    hyperparams['test_sub_idx']=[str(ii) for ii in test_index]\n",
    "    xy_train=[norm_subs_data['sub_'+str(ii)] for ii in train_index]\n",
    "    xy_test=[norm_subs_data['sub_'+str(ii)] for ii in test_index]\n",
    "    \n",
    "    xy_train=np.concatenate(xy_train,axis=0)\n",
    "    xy_test=np.concatenate(xy_test,axis=0)\n",
    "    xy_valid=xy_test\n",
    "    \n",
    "    #1) load dataset\n",
    "    train_set = windowed_dataset(xy_train, hyperparams['window_size'], hyperparams['batch_size'], shuffle_buffer=1000)\n",
    "    valid_set = windowed_dataset(xy_valid, hyperparams['window_size'], hyperparams['batch_size'], shuffle_buffer=1000)\n",
    "    print(\"Train set shape\",xy_train.shape)\n",
    "    print(\"Valid set shape\",xy_valid.shape)\n",
    "    print(\"Test set shape\",xy_test.shape)\n",
    "    #print(\"X Shape for a iteration train\",list(train_set.as_numpy_iterator())[0][0].shape)\n",
    "    #print(\"Y Shape for a iteration train\",list(train_set.as_numpy_iterator())[0][1].shape)\n",
    "\n",
    "    #2) declare model\n",
    "    model=model_v1(hyperparams)\n",
    "    \n",
    "    #3) train model\n",
    "    trained_model,history_dict,training_folder=train_model(model,hyperparams,train_set,valid_set)\n",
    "    \n",
    "    #4) test model\n",
    "    features, labels, predictions, testing_folder = test_model(training_folder,xy_test,scaler)\n",
    "    log_dict['training_folder'].append(training_folder)\n",
    "    log_dict['testing_folder'].append(testing_folder)\n",
    "     \n",
    "    # Plot\n",
    "    #plot_prediction(features,labels,predictions,testing_folder,hyperparams)\n",
    "\n",
    "\n",
    "#5) save train and test folder path\n",
    "with open(train_test_folder_log,'w') as fd:\n",
    "    yaml.dump(log_dict,fd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cpus=tf.config.list_logical_devices(device_type='CPU')\n",
    "gpus=tf.config.list_logical_devices(device_type='GPU')\n",
    "print(cpus,gpus)\n",
    "\n",
    "print(hyperparams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "history_file='/home/sun/Downloads/my_history_sub_0_1_2_3_5'\n",
    "with open(history_file,'r') as fd:\n",
    "    his=json.load(fd)\n",
    "    \n",
    "plot_history(history_dict)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "“S+P Week 4 Exercise Answer.ipynb”的副本",
   "provenance": [
    {
     "file_id": "https://github.com/lmoroney/dlaicourse/blob/master/TensorFlow%20In%20Practice/Course%204%20-%20S%2BP/S%2BP%20Week%204%20Exercise%20Answer.ipynb",
     "timestamp": 1626510374525
    }
   ],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
