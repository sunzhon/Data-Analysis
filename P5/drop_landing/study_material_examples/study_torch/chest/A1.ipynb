{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cc3578bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#1 加载必要的库\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchvision  import datasets, transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "26b145e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "#2 定义超参数\n",
    "BATCH_SIZE = 16 #每批处理的数据\n",
    "DEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "EPOCHS = 10 # 训练数据集的轮次\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "2f14bce0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#3 构建pipeline, transform 对数据作处理\n",
    "pipeline=transforms.Compose([\n",
    "    transforms.ToTensor(),#\n",
    "    transforms.Normalize((0.1307,0),(0.3081,))#正则化，降低模型复杂度，计算量，过拟合\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "eb75631a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/sun/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torchvision/datasets/mnist.py:498: UserWarning: The given NumPy array is not writeable, and PyTorch does not support non-writeable tensors. This means you can write to the underlying (supposedly non-writeable) NumPy array using the tensor. You may want to copy the array to protect its data or make it writeable before converting it to a tensor. This type of warning will be suppressed for the rest of this program. (Triggered internally at  /pytorch/torch/csrc/utils/tensor_numpy.cpp:180.)\n",
      "  return torch.from_numpy(parsed.astype(m[2], copy=False)).view(*s)\n"
     ]
    }
   ],
   "source": [
    "#4 下载、加载数据\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "#下载数据\n",
    "train_set = datasets.MNIST(\"data\",train=True,download=True,transform=pipeline)\n",
    "test_set = datasets.MNIST(\"data\",train=False,download=True,transform=pipeline)\n",
    "\n",
    "#加载数据\n",
    "\n",
    "train_loader=DataLoader(train_set,batch_size=BATCH_SIZE,shuffle=True)\n",
    "test_loader=DataLoader(test_set,batch_size=BATCH_SIZE,shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d0051125",
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(\"./data/MNIST/raw/train-images-idx3-ubyte\",\"rb\") as f:\n",
    "    file=f.read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "26f0d5cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "imagel=[int(str(item).encode('ascii'),16) for item in file[16:16+784]]\n",
    "#print(imagel.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "022d563e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(784,)\n",
      "(28, 28, 1)\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "print(np.array(imagel).shape)\n",
    "imagel_np=np.array(imagel,dtype=np.uint8).reshape(28,28,1)\n",
    "plt.imshow(imagel_np)\n",
    "print(imagel_np.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "917da189",
   "metadata": {},
   "outputs": [],
   "source": [
    "#5 构建网络模型\n",
    "class Digit(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1=nn.Conv2d(1,10,5)#灰度\n",
    "        self.conv2=nn.Conv2d(10,20,3)\n",
    "        self.fc1=nn.Linear(20*10*10,500)\n",
    "        self.fc2=nn.Linear(500,10)\n",
    "    \n",
    "    def forward(self,x):\n",
    "        input_size=x.size(0) # batch_size,1->灰度,28*28->像素\n",
    "        x=self.conv1(x) #输入：batch_size*1*28*28,输出：batch_size*10*24*24 (28-5+1)\n",
    "        x=F.relu(x)\n",
    "        x=F.max_pool2d(x,2,2) #输入: batch_size*10*24*24, 输出:batch_sze*10*12*12\n",
    "        \n",
    "        x=self.conv2(x)#输出batch * 20*10*10\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=x.view(input_size,-1)#输出batach * 20*10*10\n",
    "        \n",
    "        x=self.fc1(x) #输入 batch * 2000, 输出：batch * 500\n",
    "        x=F.relu(x)\n",
    "        \n",
    "        x=self.fc2(x)\n",
    "        output=F.log_softmax(x,dim=1)\n",
    "        \n",
    "        return output\n",
    "            \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "cdcc72d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "#6 定义优化器 \n",
    "model=Digit().to(DEVICE)\n",
    "optimizer=optim.Adam(model.parameters())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0c97c194",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([60000, 28, 28])\n",
      "tensor([5, 0, 4,  ..., 5, 6, 8])\n"
     ]
    }
   ],
   "source": [
    "print(train_loader.dataset.data.shape)\n",
    "print(train_loader.dataset.targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "f4313628",
   "metadata": {},
   "outputs": [],
   "source": [
    "#7 定义训练方法\n",
    "def train_model(model,device,train_loader, optimizer, epoch):\n",
    "    model.train()\n",
    "    print(train_loader)\n",
    "    print(next(iter(train_loader)))\n",
    "    #for batch_index, (data, target) in enumerate(train_loader):\n",
    "    for batch_index, (data, target) in enumerate(train_loader):\n",
    "        # employ data to device\n",
    "        data,target=data.to(device),target.to(device)\n",
    "        # initilize parameters\n",
    "        optimizer.zero_grad()\n",
    "        # forward calculate\n",
    "        output = model(data)\n",
    "        # calculate cost\n",
    "        loss = F.cross_entropy(output,target)        \n",
    "        loss.backward()\n",
    "        #\n",
    "        optimizer.step()\n",
    "        if batch_index %3000 ==0:\n",
    "            print(\"train epoch: {}\\t Loss:{:.6f}\\f\".format(epoch, loss.item()))\n",
    "        \n",
    "        \n",
    "        \n",
    "        \n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a5e0e5bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "#8 定义测试方法\n",
    "def test_model(model, device,test_loader):\n",
    "    #evaluate model\n",
    "    model.eval()\n",
    "    correct=0.0\n",
    "    test_loss=0.0\n",
    "    with torch.no_grad():\n",
    "        for data, target in test_loader:\n",
    "            data,target = data.to(device),target.to(device)\n",
    "            \n",
    "            output=model(data)\n",
    "            test_loss+=F.cross_entropy(output,target).item()\n",
    "            pred=output.max(axis=1,keepdim=True)[1]\n",
    "            \n",
    "            correct+=pred.eq(target.view_as(pred)).sum().item()\n",
    "        test_loss/=len(test_loader.dataset)\n",
    "        print(\"test --- Average loss: {:.4f}. Accurary: {:.3f}\\n\".format(test_loss, 100*corret/len(test_loader.dataset)))\n",
    "            \n",
    "            \n",
    "            \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "134f8c28",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<torch.utils.data.dataloader.DataLoader object at 0x7f819924edf0>\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "output with shape [1, 28, 28] doesn't match the broadcast shape [2, 28, 28]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-7122234d5f6b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mepoch\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mEPOCHS\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m     \u001b[0mtrain_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mepoch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      3\u001b[0m     \u001b[0mtest_model\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mDEVICE\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-19-f5d160124a35>\u001b[0m in \u001b[0;36mtrain_model\u001b[0;34m(model, device, train_loader, optimizer, epoch)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0miter\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;31m#for batch_index, (data, target) in enumerate(train_loader):\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32min\u001b[0m \u001b[0menumerate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_loader\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    519\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_sampler_iter\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    520\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reset\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 521\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    522\u001b[0m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    523\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    559\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    560\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 561\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    562\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    563\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torchvision/datasets/mnist.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m    132\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    133\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 134\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    135\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    136\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtarget_transform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, img)\u001b[0m\n\u001b[1;32m     58\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     59\u001b[0m         \u001b[0;32mfor\u001b[0m \u001b[0mt\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransforms\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 60\u001b[0;31m             \u001b[0mimg\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mt\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     61\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mimg\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torch/nn/modules/module.py\u001b[0m in \u001b[0;36m_call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1049\u001b[0m         if not (self._backward_hooks or self._forward_hooks or self._forward_pre_hooks or _global_backward_hooks\n\u001b[1;32m   1050\u001b[0m                 or _global_forward_hooks or _global_forward_pre_hooks):\n\u001b[0;32m-> 1051\u001b[0;31m             \u001b[0;32mreturn\u001b[0m \u001b[0mforward_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1052\u001b[0m         \u001b[0;31m# Do not call functions when jit is used\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1053\u001b[0m         \u001b[0mfull_backward_hooks\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnon_full_backward_hooks\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torchvision/transforms/transforms.py\u001b[0m in \u001b[0;36mforward\u001b[0;34m(self, tensor)\u001b[0m\n\u001b[1;32m    219\u001b[0m             \u001b[0mTensor\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mNormalized\u001b[0m \u001b[0mTensor\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    220\u001b[0m         \"\"\"\n\u001b[0;32m--> 221\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mF\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnormalize\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtensor\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minplace\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    222\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    223\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__repr__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/DataVisualization/lib/python3.9/site-packages/torchvision/transforms/functional.py\u001b[0m in \u001b[0;36mnormalize\u001b[0;34m(tensor, mean, std, inplace)\u001b[0m\n\u001b[1;32m    333\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mndim\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    334\u001b[0m         \u001b[0mstd\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mstd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mview\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 335\u001b[0;31m     \u001b[0mtensor\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msub_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmean\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdiv_\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstd\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    336\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtensor\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    337\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: output with shape [1, 28, 28] doesn't match the broadcast shape [2, 28, 28]"
     ]
    }
   ],
   "source": [
    "for epoch in range(1, EPOCHS,+1):\n",
    "    train_model(model,DEVICE,train_loader,optimizer,epoch)\n",
    "    test_model(model,DEVICE,test_loader)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
