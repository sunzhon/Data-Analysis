{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ac060d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "> \u001b[0;32m<ipython-input-2-e0caaf4db005>\u001b[0m(52)\u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;32m     50 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     51 \u001b[0;31m    \u001b[0mpdb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mset_trace\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m---> 52 \u001b[0;31m    \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubject_info\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'subject'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     53 \u001b[0;31m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\u001b[0;32m     54 \u001b[0;31m    \u001b[0mobjectName\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"s002_wangdianxin\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0m\n",
      "ipdb> subject_inf['subject']\n",
      "*** NameError: name 'subject_inf' is not defined\n",
      "ipdb> subject_info[\"subject\"]\n",
      "*** KeyError: 'subject'\n",
      "ipdb> subject_info.columns\n",
      "Index(['subject id', 'Age', 'body weight', 'body height',\n",
      "       'Caliwand for plate 1-x', 'Caliwand for plate 1-y',\n",
      "       'Caliwand for plate 1-z', 'Caliwand for plate 2-x',\n",
      "       'Caliwand for plate 2-y', 'Caliwand for plate 2-z',\n",
      "       'baseline speed(m/s^2)', 'baseline_right_fpa(degree)',\n",
      "       'baseline_left_fpa(degree)', 'baseline_step_width(m)'],\n",
      "      dtype='object')\n",
      "ipdb> subject_info[subject_id]\n",
      "*** NameError: name 'subject_id' is not defined\n",
      "ipdb> subect_info[\"subject_id\"]\n",
      "*** NameError: name 'subect_info' is not defined\n",
      "ipdb> subject_info[\"subject_id\"]\n",
      "*** KeyError: 'subject_id'\n",
      "ipdb> subject_info[\"subject id\"]\n",
      "0      s002_wangdianxin\n",
      "1        s004_ouyangjue\n",
      "2      s005_tangansheng\n",
      "3            s006_xusen\n",
      "4        s007_zuogangao\n",
      "5             s008_liyu\n",
      "6          s009_sunyubo\n",
      "7           s010_handai\n",
      "8         s011_wuxingze\n",
      "9       s012_likaixiang\n",
      "10    s013_zhangxiaohan\n",
      "11        s014_maqichao\n",
      "12         s015_weihuan\n",
      "13         s017_tantian\n",
      "14        s018_wangmian\n",
      "15    s019_chenhongyuan\n",
      "16       s020_houjikang\n",
      "17                  NaN\n",
      "18                  NaN\n",
      "19           subject id\n",
      "20         s001_tantian\n",
      "21     s002_wangdianxin\n",
      "22         s003_linyuan\n",
      "23       s004_ouyangjue\n",
      "24     s005_tangansheng\n",
      "25           s006_xusen\n",
      "26       s007_zuogangao\n",
      "27            s008_liyu\n",
      "28         s009_sunyubo\n",
      "29          s010_handai\n",
      "30        s011_wuxingze\n",
      "31      s012_likaixiang\n",
      "32    s013_zhangxiaohan\n",
      "33        s014_maqichao\n",
      "34         s015_weihuan\n",
      "35       s016_houjikang\n",
      "36         s017_tantian\n",
      "37        s018_wangmian\n",
      "38    s019_chenhongyuan\n",
      "39       s020_houjikang\n",
      "Name: subject id, dtype: object\n",
      "ipdb> subject_info['subject id'][0:17]\n",
      "0      s002_wangdianxin\n",
      "1        s004_ouyangjue\n",
      "2      s005_tangansheng\n",
      "3            s006_xusen\n",
      "4        s007_zuogangao\n",
      "5             s008_liyu\n",
      "6          s009_sunyubo\n",
      "7           s010_handai\n",
      "8         s011_wuxingze\n",
      "9       s012_likaixiang\n",
      "10    s013_zhangxiaohan\n",
      "11        s014_maqichao\n",
      "12         s015_weihuan\n",
      "13         s017_tantian\n",
      "14        s018_wangmian\n",
      "15    s019_chenhongyuan\n",
      "16       s020_houjikang\n",
      "Name: subject id, dtype: object\n",
      "ipdb> subject_info[\"subject id\"][0:16].values\n",
      "array(['s002_wangdianxin', 's004_ouyangjue', 's005_tangansheng',\n",
      "       's006_xusen', 's007_zuogangao', 's008_liyu', 's009_sunyubo',\n",
      "       's010_handai', 's011_wuxingze', 's012_likaixiang',\n",
      "       's013_zhangxiaohan', 's014_maqichao', 's015_weihuan',\n",
      "       's017_tantian', 's018_wangmian', 's019_chenhongyuan'], dtype=object)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import pdb\n",
    "import h5py\n",
    "\n",
    "import sys\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import math\n",
    "from matplotlib import gridspec\n",
    "import os\n",
    "import pdb\n",
    "import termcolor\n",
    "import gnureadline\n",
    "plt.rc('font',family='Arial')\n",
    "import pandas as pd\n",
    "import re\n",
    "import time as localtimepkg\n",
    "from brokenaxes import brokenaxes\n",
    "from mpl_toolkits import mplot3d\n",
    "from matplotlib import animation\n",
    "import seaborn as sns\n",
    "import matplotlib as mpl\n",
    "from mpl_toolkits.mplot3d import Axes3D\n",
    "import warnings\n",
    "from matplotlib.animation import FuncAnimation\n",
    "\n",
    "\n",
    "# colunms=\"Event AccelX_L_FOOT AccelY_L_FOOT\tAccelZ_L_FOOT\tGyroX_L_FOOT\tGyroY_L_FOOT\tGyroZ_L_FOOT\tMagX_L_FOOT\tMagY_L_FOOT\tMagZ_L_FOOT\tQuat1_L_FOOT\tQuat2_L_FOOT\tQuat3_L_FOOT\tQuat4_L_FOOT\tAccelX_R_FOOT\tAccelY_R_FOOT\tAccelZ_R_FOOT\tGyroX_R_FOOT\tGyroY_R_FOOT\tGyroZ_R_FOOT\tMagX_R_FOOT\tMagY_R_FOOT\tMagZ_R_FOOT\tQuat1_R_FOOT\tQuat2_R_FOOT\tQuat3_R_FOOT\tQuat4_R_FOOT\tAccelX_R_SHANK\tAccelY_R_SHANK\tAccelZ_R_SHANK\tGyroX_R_SHANK\tGyroY_R_SHANK\tGyroZ_R_SHANK\tMagX_R_SHANK\tMagY_R_SHANK\tMagZ_R_SHANK\tQuat1_R_SHANK\tQuat2_R_SHANK\tQuat3_R_SHANK\tQuat4_R_SHANK\tAccelX_R_THIGH\tAccelY_R_THIGH\tAccelZ_R_THIGH\tGyroX_R_THIGH\tGyroY_R_THIGH\tGyroZ_R_THIGH\tMagX_R_THIGH\tMagY_R_THIGH\tMagZ_R_THIGH\tQuat1_R_THIGH\tQuat2_R_THIGH\tQuat3_R_THIGH\tQuat4_R_THIGH\tAccelX_WAIST\tAccelY_WAIST\tAccelZ_WAIST\tGyroX_WAIST\tGyroY_WAIST\tGyroZ_WAIST\tMagX_WAIST\tMagY_WAIST\tMagZ_WAIST\tQuat1_WAIST\tQuat2_WAIST\tQuat3_WAIST\tQuat4_WAIST\tAccelX_CHEST\tAccelY_CHEST\tAccelZ_CHEST\tGyroX_CHEST\tGyroY_CHEST\tGyroZ_CHEST\tMagX_CHEST\tMagY_CHEST\tMagZ_CHEST\tQuat1_CHEST\tQuat2_CHEST\tQuat3_CHEST\tQuat4_CHEST\tAccelX_L_SHANK\tAccelY_L_SHANK\tAccelZ_L_SHANK\tGyroX_L_SHANK\tGyroY_L_SHANK\tGyroZ_L_SHANK\tMagX_L_SHANK\tMagY_L_SHANK\tMagZ_L_SHANK\tQuat1_L_SHANK\tQuat2_L_SHANK\tQuat3_L_SHANK\tQuat4_L_SHANK\tAccelX_L_THIGH\tAccelY_L_THIGH\tAccelZ_L_THIGH\tGyroX_L_THIGH\tGyroY_L_THIGH\tGyroZ_L_THIGH\tMagX_L_THIGH\tMagY_L_THIGH\tMagZ_L_THIGH\tQuat1_L_THIGH\tQuat2_L_THIGH\tQuat3_L_THIGH\tQuat4_L_THIGH\tNose_x_90\tNose_y_90\tNose_probability_90\tNeck_x_90\tNeck_y_90\tNeck_probability_90\tRShoulder_x_90\tRShoulder_y_90\tRShoulder_probability_90\tRElbow_x_90\tRElbow_y_90\tRElbow_probability_90\tRWrist_x_90\tRWrist_y_90\tRWrist_probability_90\tLShoulder_x_90\tLShoulder_y_90\tLShoulder_probability_90\tLElbow_x_90\tLElbow_y_90\tLElbow_probability_90\tLWrist_x_90\tLWrist_y_90\tLWrist_probability_90\tMidHip_x_90\tMidHip_y_90\tMidHip_probability_90\tRHip_x_90\tRHip_y_90\tRHip_probability_90\tRKnee_x_90\tRKnee_y_90\tRKnee_probability_90\tRAnkle_x_90\tRAnkle_y_90\tRAnkle_probability_90\tLHip_x_90\tLHip_y_90\tLHip_probability_90\tLKnee_x_90\tLKnee_y_90\tLKnee_probability_90\tLAnkle_x_90\tLAnkle_y_90\tLAnkle_probability_90\tREye_x_90\tREye_y_90\tREye_probability_90\tLEye_x_90\tLEye_y_90\tLEye_probability_90\tREar_x_90\tREar_y_90\tREar_probability_90\tLEar_x_90\tLEar_y_90\tLEar_probability_90\tLBigToe_x_90\tLBigToe_y_90\tLBigToe_probability_90\tLSmallToe_x_90\tLSmallToe_y_90\tLSmallToe_probability_90\tLHeel_x_90\tLHeel_y_90\tLHeel_probability_90\tRBigToe_x_90\tRBigToe_y_90\tRBigToe_probability_90\tRSmallToe_x_90\tRSmallToe_y_90\tRSmallToe_probability_90\tRHeel_x_90\tRHeel_y_90\tRHeel_probability_90\tNose_x_180\tNose_y_180\tNose_probability_180\tNeck_x_180\tNeck_y_180\tNeck_probability_180\tRShoulder_x_180\tRShoulder_y_180\tRShoulder_probability_180\tRElbow_x_180\tRElbow_y_180\tRElbow_probability_180\tRWrist_x_180\tRWrist_y_180\tRWrist_probability_180\tLShoulder_x_180\tLShoulder_y_180\tLShoulder_probability_180\tLElbow_x_180\tLElbow_y_180\tLElbow_probability_180\tLWrist_x_180\tLWrist_y_180\tLWrist_probability_180\tMidHip_x_180\tMidHip_y_180\tMidHip_probability_180\tRHip_x_180\tRHip_y_180\tRHip_probability_180\tRKnee_x_180\tRKnee_y_180\tRKnee_probability_180\tRAnkle_x_180\tRAnkle_y_180\tRAnkle_probability_180\tLHip_x_180\tLHip_y_180\tLHip_probability_180\tLKnee_x_180\tLKnee_y_180\tLKnee_probability_180\tLAnkle_x_180\tLAnkle_y_180\tLAnkle_probability_180\tREye_x_180\tREye_y_180\tREye_probability_180\tLEye_x_180\tLEye_y_180\tLEye_probability_180\tREar_x_180\tREar_y_180\tREar_probability_180\tLEar_x_180\tLEar_y_180\tLEar_probability_180\tLBigToe_x_180\tLBigToe_y_180\tLBigToe_probability_180\tLSmallToe_x_180\tLSmallToe_y_180\tLSmallToe_probability_180\tLHeel_x_180\tLHeel_y_180\tLHeel_probability_180\tRBigToe_x_180\tRBigToe_y_180\tRBigToe_probability_180\tRSmallToe_x_180\tRSmallToe_y_180\tRSmallToe_probability_180\tRHeel_x_180\tRHeel_y_180\tRHeel_probability_180\tRIGHT_KNEE_ADDUCTION_MOMENT\tRIGHT_KNEE_FLEXION_MOMENT\tRIGHT_KNEE_ADDUCTION_ANGLE\tRIGHT_KNEE_ADDUCTION_VELOCITY\tLFCC_X\tLFCC_Y\tLFCC_Z\tLFM5_X\tLFM5_Y\tLFM5_Z\tLFM2_X\tLFM2_Y\tLFM2_Z\tRFCC_X\tRFCC_Y\tRFCC_Z\tRFM5_X\tRFM5_Y\tRFM5_Z\tRFM2_X\tRFM2_Y\tRFM2_Z\tLTAM_X\tLTAM_Y\tLTAM_Z\tLFAL_X\tLFAL_Y\tLFAL_Z\tLSK_X\tLSK_Y\tLSK_Z\tLTT_X\tLTT_Y\tLTT_Z\tRTAM_X\tRTAM_Y\tRTAM_Z\tRFAL_X\tRFAL_Y\tRFAL_Z\tRSK_X\tRSK_Y\tRSK_Z\tRTT_X\tRTT_Y\tRTT_Z\tLFME_X\tLFME_Y\tLFME_Z\tLFLE_X\tLFLE_Y\tLFLE_Z\tLTH_X\tLTH_Y\tLTH_Z\tLFT_X\tLFT_Y\tLFT_Z\tRFME_X\tRFME_Y\tRFME_Z\tRFLE_X\tRFLE_Y\tRFLE_Z\tRTH_X\tRTH_Y\tRTH_Z\tRFT_X\tRFT_Y\tRFT_Z\tLIPS_X\tLIPS_Y\tLIPS_Z\tRIPS_X\tRIPS_Y\tRIPS_Z\tLIAS_X\tLIAS_Y\tLIAS_Z\tRIAS_X\tRIAS_Y\tRIAS_Z\tMAI_X\tMAI_Y\tMAI_Z\tSXS_X\tSXS_Y\tSXS_Z\tSJN_X\tSJN_Y\tSJN_Z\tCV7_X\tCV7_Y\tCV7_Z\tLAC_X\tLAC_Y\tLAC_Z\tRAC_X\tRAC_Y\tRAC_Z\tplate_1_force_x\tplate_1_force_y\tplate_1_force_z\tplate_1_cop_x\tplate_1_cop_y\tplate_1_cop_z\tplate_2_force_x\tplate_2_force_y\tplate_2_force_z\tplate_2_cop_x\tplate_2_cop_y\tplate_2_cop_z\tEXT_KM_X EXT_KM_Y EXT_KM_Z body height body weight subject_id trial_id\"\n",
    "\n",
    "\n",
    "def loadData(objectName,folderName=\"/media/suntao/DATA/Research/Human_walking_date/processed_data\"):\n",
    "    '''  \n",
    "    load data from a file\n",
    "    fileName: the name of file that you want to read\n",
    "    columnsName: it the column name of the file\n",
    "    Note: the args of sys is file_id and date of the file\n",
    "    ''' \n",
    "    #1) load data from file\n",
    "    data_file = folderName +\"/\"+ objectName + \"/combined/baseline.csv\"\n",
    "    resource_data = pd.read_csv(data_file, index_col=0,header=0,skip_blank_lines=True,dtype=str)\n",
    "    read_rows=resource_data.shape[0]-1\n",
    "    fine_data = resource_data.iloc[0:read_rows,:].astype(float)# 数据行对齐\n",
    "    return fine_data\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    data_file=\"/media/suntao/DATA/Research/Human_walking_date/processed_data/subject_info.csv\"\n",
    "    subject_info = pd.read_csv(data_file, index_col=None,header=1,skip_blank_lines=False,dtype=str)\n",
    "    subjects=subject_info['subject id'][0:16].values\n",
    "    \n",
    "    for objectName in subjects:\n",
    "        print(objectName)\n",
    "        pdb.set_trace()\n",
    "        objectName=\"s002_wangdianxin\"\n",
    "        dataset=loadData(objectName)\n",
    "        columns=dataset.columns.values\n",
    "        print(columns)\n",
    "        print(dataset.shape)\n",
    "        #print(dataset[\"plate_1_force_x\"])\n",
    "        #plt.plot(dataset['plate_1_force_x'][200:],'r')\n",
    "        #lt.plot(dataset['plate_1_force_y'][200:],\"g\")\n",
    "        #plt.plot(dataset['plate_1_force_z'][1000:1400],\"b\")\n",
    "        \n",
    "        fileName=\"/media/suntao/DATA/Research/Human_walking_date/processed_data/suntao_all_17_subjects.h5\"\n",
    "        with h5py.File(fileName, \"w\") as h5_data_set:\n",
    "            objects_group=h5_data_set.create_group(\"objects_group\")\n",
    "            objects_group.create_dataset(name=\"object_01\",data=dataset.values)\n",
    "            print(h5_data_set.keys())\n",
    "            print(h5_data_set['objects_group'])\n",
    "            print(h5_data_set['objects_group']['object_01'].shape)\n",
    "            h5_data_set[\"objects_group\"]['object_01'].attrs.create(\"columns\",columns)\n",
    "            #pdb.set_trace()\n",
    "            #data_set['subject_01']\n",
    "            \n",
    "                                     \n",
    "        print(\"------------------------------------\")\n",
    "        fileName=\"/media/suntao/DATA/Research/Human_walking_date/processed_data/all_17_subjects.h5\"\n",
    "        with h5py.File(fileName, \"r\") as data_set:\n",
    "            print(data_set.keys())\n",
    "            #pdb.set_trace()\n",
    "            print(data_set['subject_01'])\n",
    "            print(data_set['subject_01'].shape)\n",
    "            print(data_set['subject_01'].size)\n",
    "            print(data_set['subject_01'].dtype)\n",
    "            print(data_set['subject_01'].ndim)\n",
    "            print(data_set['subject_01'].attrs)\n",
    "            print(data_set['subject_01'].attrs.keys())\n",
    "            print(data_set['subject_01'].attrs.values())\n",
    "            print(data_set['subject_01'].attrs.items())\n",
    "            \n",
    "            print(data_set.attrs.keys())\n",
    "            \n",
    "            print(data_set['subject_01'][0,0,0])\n",
    "            \n",
    "            print(len(data_set.attrs.items()))\n",
    "            if len(data_set.attrs.items()):\n",
    "                print(\"{} contains: \".format(fileName))\n",
    "                print(\"Root attributes:\")\n",
    "        \n",
    "            for key, value in data_set.attrs.items():\n",
    "                print(\" {}: {}\".format(key, value)) # 输出储存在File类中的attrs信息，一般是各层的名称\n",
    "      \n",
    "            print(value)\n",
    "            for layer, g in data_set.items(): # 读取各层的名称以及包含层信息的Group类\n",
    "                print(\" {}\".format(layer))\n",
    "                print(\"  Attributes:\")\n",
    "                for key, value in g.attrs.items(): # 输出储存在Group类中的attrs信息，一般是各层的weights和bias及他们的名称\n",
    "                    print(\"   {}: {}\".format(key, value)) \n",
    "                    print(\"sss:{}\".format(len(value)))\n",
    "                print(\"  Dataset:\")\n",
    "                '''\n",
    "                print(g['body_weight'])\n",
    "                for name, d in g.items(): # 读取各层储存具体信息的Dataset类\n",
    "                    print(\"   {}: {}\".format(name, d.value.shape)) # 输出储存在Dataset中的层名称和权重，也可以打印dataset的attrs，但是keras中是空的\n",
    "                    print(\"   {}: {}\".format(name. d.value))\n",
    "                '''\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab9fb0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7603e77",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import pdb\n",
    "\n",
    "# Fetch and format the mnist data\n",
    "(mnist_images, mnist_labels), _ = tf.keras.datasets.mnist.load_data()\n",
    "\n",
    "dataset = tf.data.Dataset.from_tensor_slices(\n",
    "  (tf.cast(mnist_images[...,tf.newaxis]/255, tf.float32),\n",
    "   tf.cast(mnist_labels,tf.int64)))\n",
    "dataset = dataset.shuffle(1000).batch(32)\n",
    "\n",
    "\n",
    "\n",
    "# Build the model\n",
    "mnist_model = tf.keras.Sequential([\n",
    "    tf.keras.layers.Conv2D(16, [3,3], activation=\"relu\",\n",
    "                          input_shape=(None, None, 1)),\n",
    "    tf.keras.layers.Conv2D(16,[3,3],activation=\"relu\"),\n",
    "    tf.keras.layers.GlobalAveragePooling2D(),\n",
    "    tf.keras.layers.Dense(10)\n",
    "],name=\"mnist_model\")\n",
    "\n",
    "mnist_model.summary()\n",
    "\n",
    "for images,labels in dataset.take(1):\n",
    "  print(\"Logits: \", mnist_model(images[0:1]).numpy())\n",
    "  pdb.set_trace()\n",
    "\n",
    "\n",
    "optimizer = tf.keras.optimizers.Adam()\n",
    "loss_object = tf.keras.losses.SparseCategoricalCrossentropy(from_logits=True)\n",
    "loss_history = []\n",
    "\n",
    "\n",
    "def train_step(images, labels):\n",
    "  with tf.GradientTape() as tape:\n",
    "    logits = mnist_model(images, training=True)\n",
    "\n",
    "    # Add asserts to check the shape of the output.\n",
    "    tf.debugging.assert_equal(logits.shape, (32, 10))\n",
    "\n",
    "    loss_value = loss_object(labels, logits)\n",
    "\n",
    "  loss_history.append(loss_value.numpy().mean())\n",
    "  grads = tape.gradient(loss_value, mnist_model.trainable_variables)\n",
    "  optimizer.apply_gradients(zip(grads, mnist_model.trainable_variables))\n",
    "\n",
    "def train(epochs):\n",
    "  for epoch in range(epochs):\n",
    "    for (batch, (images, labels)) in enumerate(dataset):\n",
    "      train_step(images, labels)\n",
    "    print ('Epoch {} finished'.format(epoch))\n",
    "    \n",
    "\n",
    "#train(epochs = 1)\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "#plt.plot(loss_history)\n",
    "#plt.xlabel('Batch #')\n",
    "#plt.ylabel('Loss [entropy]')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
